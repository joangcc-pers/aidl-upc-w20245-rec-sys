{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e6e2e56448db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/raw/2019-Oct.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raw_data = pd.read_csv(\"../data/raw/2019-Oct.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>event_type</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_code</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-01 00:00:00 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>44600062</td>\n",
       "      <td>2103807459595387724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shiseido</td>\n",
       "      <td>35.79</td>\n",
       "      <td>541312140</td>\n",
       "      <td>72d76fde-8bb3-4e00-8c23-a032dfed738c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-01 00:00:00 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>3900821</td>\n",
       "      <td>2053013552326770905</td>\n",
       "      <td>appliances.environment.water_heater</td>\n",
       "      <td>aqua</td>\n",
       "      <td>33.20</td>\n",
       "      <td>554748717</td>\n",
       "      <td>9333dfbd-b87a-4708-9857-6336556b0fcc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-01 00:00:01 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>17200506</td>\n",
       "      <td>2053013559792632471</td>\n",
       "      <td>furniture.living_room.sofa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>543.10</td>\n",
       "      <td>519107250</td>\n",
       "      <td>566511c2-e2e3-422b-b695-cf8e6e792ca8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-01 00:00:01 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>1307067</td>\n",
       "      <td>2053013558920217191</td>\n",
       "      <td>computers.notebook</td>\n",
       "      <td>lenovo</td>\n",
       "      <td>251.74</td>\n",
       "      <td>550050854</td>\n",
       "      <td>7c90fc70-0e80-4590-96f3-13c02c18c713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-01 00:00:04 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>1004237</td>\n",
       "      <td>2053013555631882655</td>\n",
       "      <td>electronics.smartphone</td>\n",
       "      <td>apple</td>\n",
       "      <td>1081.98</td>\n",
       "      <td>535871217</td>\n",
       "      <td>c6bd7419-2748-4c56-95b4-8cec9ff8b80d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                event_time event_type  product_id          category_id  \\\n",
       "0  2019-10-01 00:00:00 UTC       view    44600062  2103807459595387724   \n",
       "1  2019-10-01 00:00:00 UTC       view     3900821  2053013552326770905   \n",
       "2  2019-10-01 00:00:01 UTC       view    17200506  2053013559792632471   \n",
       "3  2019-10-01 00:00:01 UTC       view     1307067  2053013558920217191   \n",
       "4  2019-10-01 00:00:04 UTC       view     1004237  2053013555631882655   \n",
       "\n",
       "                         category_code     brand    price    user_id  \\\n",
       "0                                  NaN  shiseido    35.79  541312140   \n",
       "1  appliances.environment.water_heater      aqua    33.20  554748717   \n",
       "2           furniture.living_room.sofa       NaN   543.10  519107250   \n",
       "3                   computers.notebook    lenovo   251.74  550050854   \n",
       "4               electronics.smartphone     apple  1081.98  535871217   \n",
       "\n",
       "                           user_session  \n",
       "0  72d76fde-8bb3-4e00-8c23-a032dfed738c  \n",
       "1  9333dfbd-b87a-4708-9857-6336556b0fcc  \n",
       "2  566511c2-e2e3-422b-b695-cf8e6e792ca8  \n",
       "3  7c90fc70-0e80-4590-96f3-13c02c18c713  \n",
       "4  c6bd7419-2748-4c56-95b4-8cec9ff8b80d  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "event_time              0\n",
       "event_type              0\n",
       "product_id              0\n",
       "category_id             0\n",
       "category_code    13515609\n",
       "brand             6117080\n",
       "price                   0\n",
       "user_id                 0\n",
       "user_session            2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_time        0.000000\n",
      "event_type        0.000000\n",
      "product_id        0.000000\n",
      "category_id       0.000000\n",
      "category_code    31.839818\n",
      "brand            14.410502\n",
      "price             0.000000\n",
      "user_id           0.000000\n",
      "user_session      0.000005\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "percentage_missing_per_column = raw_data.isna().mean() * 100\n",
    "\n",
    "print(percentage_missing_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.42899086531707\n"
     ]
    }
   ],
   "source": [
    "percentage_rows_with_any_na = (raw_data.isna().any(axis=1).mean()) * 100\n",
    "\n",
    "print(percentage_rows_with_any_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Group by category_id\n",
    "grouped = raw_data.groupby('category_id')['category_code']\n",
    "\n",
    "# Check if both NA and non-NA values exist for any group\n",
    "result = grouped.apply(lambda x: x.isna().any() and x.notna().any()).any()\n",
    "\n",
    "#Show the results (if any)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.10.13' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from scripts.read import NextItemDataset  # Assuming the class is in scripts/read.py\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = NextItemDataset(\n",
    "    path='path/to/your/data.csv',  # Replace with your CSV path\n",
    "    mode='gru',  # or 'graph'\n",
    "    max_seq_len=50  # Optional: set maximum sequence length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/miguelpalospou/Desktop/Data_projects/master_project/aidl-upc-w20245-rec-sys/notebooks\n",
      "\n",
      "Contents of current directory: ['umap_visualization.ipynb', 'data_exploration.ipynb', 'experiments', 'performance_comparison.ipynb']\n",
      "\n",
      "scripts directory not found in: /Users/miguelpalospou/Desktop/Data_projects/master_project/aidl-upc-w20245-rec-sys/notebooks/scripts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# List contents of current directory\n",
    "print(\"\\nContents of current directory:\", os.listdir())\n",
    "\n",
    "# Try to list contents of scripts directory if it exists\n",
    "scripts_path = os.path.join(os.getcwd(), 'scripts')\n",
    "if os.path.exists(scripts_path):\n",
    "    print(\"\\nContents of scripts directory:\", os.listdir(scripts_path))\n",
    "else:\n",
    "    print(\"\\nscripts directory not found in:\", scripts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to Python path: /Users/miguelpalospou/Desktop/Data_projects/master_project/aidl-upc-w20245-rec-sys\n",
      "Applying preprocessing for Graph NN with embeddings for categorical features...\n",
      "[INFO] Initializing SessionGraphDataset...\n",
      "[INFO] Loading CSV files from folder: /Users/miguelpalospou/Desktop/Data_projects/master_project/\n",
      "[INFO] Loaded 42448764 rows of data from 1 CSV files.\n",
      "[INFO] Normalizing price column using min_max...\n",
      "[INFO] Data limited to 40779399 rows with event_type = 'view'.\n",
      "[INFO] Data limited to 25201706 rows with no nulls in 'brand', 'category_code' and 'price'.\n",
      "[INFO] Data limited to 16967752 rows with 3 or more unique products per session.\n",
      "[INFO:TEST_MODE] Limiting data to the first 10000 sessions...\n",
      "[INFO:TEST_MODE] Data limited to 89542 rows from the first 10000 sessions.\n",
      "[INFO] Parsing category hierarchy...\n",
      "          category sub_category  element\n",
      "0        computers     notebook  unknown\n",
      "1        computers      desktop  unknown\n",
      "2      electronics   smartphone  unknown\n",
      "3        furniture      bedroom      bed\n",
      "4       appliances      kitchen    mixer\n",
      "...            ...          ...      ...\n",
      "89537   appliances      kitchen     hood\n",
      "89538   appliances      kitchen     hood\n",
      "89539   appliances      kitchen     hood\n",
      "89540   appliances      kitchen     hood\n",
      "89541  electronics        video       tv\n",
      "\n",
      "[89542 rows x 3 columns]\n",
      "[INFO] Parsed category hierarchy into 'category', 'sub_category', and 'element' columns.\n",
      "Category after parsing\n",
      "['computers' 'electronics' 'furniture' 'appliances' 'apparel'\n",
      " 'construction' 'auto' 'kids' 'accessories' 'sport' 'country_yard'\n",
      " 'stationery' 'medicine']\n",
      "['notebook' 'desktop' 'smartphone' 'bedroom' 'kitchen' 'shoes'\n",
      " 'environment' 'tools' 'video' 'clocks' 'components' 'audio' 'tablet'\n",
      " 'sewing_machine' 'peripherals' 'accessories' 'bathroom' 'ironing_board'\n",
      " 'personal' 'skates' 'living_room' 'iron' 'bag' 'telephone' 'carriage'\n",
      " 'underwear' 'bicycle' 'furniture' 'wallet' 'camera' 'toys' 'universal'\n",
      " 'umbrella' 'fmcg' 'jacket' 'dress' 'trousers' 'ski' 'swing' 'cultivator'\n",
      " 'cartrige' 'shirt' 'dolls' 'ebooks' 'trainer' 'snowboard' 'lawn_mower'\n",
      " 'scarf' 'sock' 'tshirt' 'belt' 'skirt' 'costume']\n",
      "['unknown' 'bed' 'mixer' 'vacuum' 'pump' 'tv' 'keds' 'refrigerators'\n",
      " 'videocards' 'drill' 'headphone' 'washer' 'dishwasher' 'air_heater'\n",
      " 'air_conditioner' 'water_heater' 'kettle' 'monitor' 'printer' 'alarm'\n",
      " 'toilet' 'player' 'microwave' 'saw' 'hair_cutter' 'welding'\n",
      " 'meat_grinder' 'power_supply' 'cabinet' 'subwoofer' 'generator' 'slipons'\n",
      " 'grill' 'blender' 'cpu' 'motherboard' 'hob' 'faucet' 'table' 'radar'\n",
      " 'hdd' 'projector' 'moccasins' 'memory' 'microphone' 'oven' 'compressor'\n",
      " 'hammok' 'keyboard' 'toster' 'chair' 'hood' 'sofa' 'mouse'\n",
      " 'videoregister' 'winch' 'sandals' 'acoustic' 'pillow' 'cooler'\n",
      " 'parktronic' 'light' 'steam_cooker' 'video' 'scales' 'blanket' 'diapers'\n",
      " 'juicer' 'step_ins' 'espadrilles' 'bath' 'coffee_machine'\n",
      " 'coffee_grinder' 'ballet_shoes' 'painting' 'photo' 'music_tools'\n",
      " 'massager' 'tonometer' 'camera' 'fan']\n",
      "[INFO] Sorting data by 'user_session' and 'event_time'...\n",
      "[INFO] Data sorted.\n",
      "[DEBUG] Unique categories count: 13\n",
      "[DEBUG] Unique sub-categories count: 53\n",
      "[DEBUG] Unique elements count: 81\n",
      "[DEBUG] Unique brands count: 936\n",
      "\n",
      "[DEBUG] Brand encoding check:\n",
      "Number of brand columns created: 936\n",
      "Sample brand columns: ['brand_a-elita', 'brand_aardwolf', 'brand_acer', 'brand_acme', 'brand_adamex']\n",
      "Sum of brand values: 89542.0\n",
      "[INFO] Categorical columns encoded.\n",
      "[DEBUG] Encoded features shape: (89542, 1083)\n",
      "[INFO] Extracting unique sessions...\n",
      "[INFO] Found 10000 unique sessions.\n",
      "[INFO] SessionGraphDataset initialization complete.\n",
      "Data(edge_index=[2, 16], y=5181, price_tensor=[9, 1], session_id='001928f2-e689-446a-a5d8-27d8e1018a1c', category=[9, 13], sub_category=[9, 53], element=[9, 81], brand=[9, 936], product_id_remapped=[9], num_nodes=9)\n",
      "product_id_remapped shape: torch.Size([9])\n",
      "category shape: torch.Size([9])\n",
      "category shape: torch.Size([9, 53])\n",
      "category shape: torch.Size([9, 81])\n",
      "category shape: torch.Size([9, 936])\n",
      "Price tensor shape: torch.Size([9, 1])\n",
      "Edge index shape: torch.Size([2, 16])\n",
      "Target shape: torch.Size([])\n",
      "Datasets saved in experiments/experiment_7/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the project root directory (one level up from notebooks) to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "print(\"Added to Python path:\", project_root)\n",
    "\n",
    "# Now try importing\n",
    "from scripts.preprocessing_scripts.preprocess_graph_with_encoding import preprocess_graph_with_onehot\n",
    "\n",
    "# Load config\n",
    "import yaml\n",
    "with open('../experiments/config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "exp_params = config['experiments']['experiment_7']\n",
    "\n",
    "# Preprocess data and create datasets\n",
    "preprocess_graph_with_onehot(\n",
    "    input_folder_path=exp_params['data_params']['input_folder_path'],\n",
    "    output_folder_artifacts=exp_params['data_params']['output_folder_artifacts'],\n",
    "    preprocessing_params=exp_params['preprocessing']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try importing\n",
    "from scripts.preprocessing_scripts.preprocess_graph_with_encoding import preprocess_graph_with_onehot\n",
    "\n",
    "# Load config\n",
    "import yaml\n",
    "with open('../experiments/config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "exp_params = config['experiments']['experiment_7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'initial_dimension_dim': 128,\n",
       " 'num_layers': 5,\n",
       " 'num_iterations': 1,\n",
       " 'hidden_units': 256,\n",
       " 'hidden_dim': 100,\n",
       " 'dropout_rate': 0.3,\n",
       " 'optimizer': 'Adam',\n",
       " 'lr': 0.0005,\n",
       " 'epochs': 15,\n",
       " 'batch_size': 32}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_params['model_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets from:\n",
      "Train: experiments/experiment_7/train_dataset.pth\n",
      "Val: experiments/experiment_7/val_dataset.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tb/spjhzdz13378nkrzqyjljkjm0000gn/T/ipykernel_50064/3928591943.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_dataset = torch.load(train_path)\n",
      "/var/folders/tb/spjhzdz13378nkrzqyjljkjm0000gn/T/ipykernel_50064/3928591943.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_dataset = torch.load(val_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset types:\n",
      "Train dataset type: <class 'torch.utils.data.dataset.Subset'>\n",
      "Val dataset type: <class 'torch.utils.data.dataset.Subset'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# First, load the datasets\n",
    "train_path = exp_params['data_params']['output_folder_artifacts'] + \"train_dataset.pth\"\n",
    "val_path = exp_params['data_params']['output_folder_artifacts'] + \"val_dataset.pth\"\n",
    "\n",
    "print(\"Loading datasets from:\")\n",
    "print(f\"Train: {train_path}\")\n",
    "print(f\"Val: {val_path}\")\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = torch.load(train_path)\n",
    "val_dataset = torch.load(val_path)\n",
    "\n",
    "print(\"\\nDataset types:\")\n",
    "print(f\"Train dataset type: {type(train_dataset)}\")\n",
    "print(f\"Val dataset type: {type(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 250/250 [02:42<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch=1/15 | Loss=2136.6003 | R@20=0.0856 | P@20=0.0043 | MRR@20=0.0218\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_scripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_sr_gnn_attn_agg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_sr_gnn_att_agg_with_onehot\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_sr_gnn_att_agg_with_onehot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_params\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_folder_artifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_params\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_folder_artifacts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Data_projects/master_project/aidl-upc-w20245-rec-sys/scripts/train_scripts/train_sr_gnn_attn_agg.py:92\u001b[0m, in \u001b[0;36mtrain_sr_gnn_att_agg_with_onehot\u001b[0;34m(model_params, train_dataset, eval_dataset, output_folder_artifacts, top_k)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Entrenamiento y evaluación por época\u001b[39;00m\n\u001b[1;32m     91\u001b[0m train_loss, train_metrics \u001b[38;5;241m=\u001b[39m train_epoch(model, train_dataloader, optimizer, criterion, total_epochs\u001b[38;5;241m=\u001b[39mepochs, current_epoch\u001b[38;5;241m=\u001b[39mepoch, top_k\u001b[38;5;241m=\u001b[39mtop_k, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 92\u001b[0m eval_loss, eval_metrics \u001b[38;5;241m=\u001b[39m \u001b[43meval_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Registrar pérdidas y métricas en TensorBoard\u001b[39;00m\n\u001b[1;32m     95\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss/Train\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loss, epoch)\n",
      "File \u001b[0;32m~/Desktop/Data_projects/master_project/aidl-upc-w20245-rec-sys/scripts/train_scripts/train_sr_gnn_attn_agg.py:209\u001b[0m, in \u001b[0;36meval_epoch\u001b[0;34m(model, eval_dataloader, criterion, total_epochs, current_epoch, top_k, device)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_epoch\u001b[39m(model, eval_dataloader, criterion, total_epochs, current_epoch, top_k\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m20\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 209\u001b[0m     all_predictions, all_targets, total_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m compute_metrics(all_predictions, all_targets, top_k)\n\u001b[1;32m    211\u001b[0m     print_metrics(total_epochs, current_epoch, top_k, total_loss, metrics, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Data_projects/master_project/aidl-upc-w20245-rec-sys/scripts/evaluate_scripts/evaluate_model_utils.py:22\u001b[0m, in \u001b[0;36mevaluate_model_epoch\u001b[0;34m(model, split_loader, criterion, device, top_k_values)\u001b[0m\n\u001b[1;32m     19\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mdataloader\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation Epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     23\u001b[0m         batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m# Get the predicted scores\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "from scripts.train_scripts.train_sr_gnn_attn_agg import train_sr_gnn_att_agg_with_onehot\n",
    "\n",
    "\n",
    "train_sr_gnn_att_agg_with_onehot(\n",
    "        model_params=exp_params['model_params'],\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        output_folder_artifacts=exp_params['data_params']['output_folder_artifacts'],\n",
    "        top_k=[20]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scripts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing_scripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocess_graph_with_encoding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess_graph_with_onehot\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load config\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiments/config.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scripts'"
     ]
    }
   ],
   "source": [
    "from scripts.preprocessing_scripts.preprocess_graph_with_encoding import preprocess_graph_with_onehot\n",
    "\n",
    "# Load config\n",
    "with open('experiments/config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "exp_params = config['experiments']['experiment_7']\n",
    "\n",
    "# Preprocess data and create datasets\n",
    "preprocess_graph_with_onehot(\n",
    "    input_folder_path=exp_params['data_params']['input_folder_path'],\n",
    "    output_folder_artifacts=exp_params['data_params']['output_folder_artifacts'],\n",
    "    preprocessing_params=exp_params['preprocessing']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aidl-session-5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
