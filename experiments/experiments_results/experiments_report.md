# Experiments report

## Graph With Embeddings

Sorted by Recall@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | R@20 | P@20 | MRR@20 | Loss |
|-------------|--------------|---------------|------|------|---------|------|
| 1e-05 | 0.0 | 0.001 | 0.4964 | 0.0248 | 0.1916 | 5.5573 |
| 1e-05 | 0.5 | 0.001 | 0.4960 | 0.0248 | 0.1931 | 5.5442 |
| 1e-06 | 0.5 | 0.001 | 0.4910 | 0.0246 | 0.1943 | 5.7627 |
| 1e-06 | 0.0 | 0.001 | 0.4904 | 0.0245 | 0.1926 | 5.7478 |
| 1e-06 | 0.2 | 0.001 | 0.4909 | 0.0245 | 0.1926 | 5.7427 |
<!--| 1e-05 | 0.2 | 0.001 | 0.4974 | 0.0249 | 0.1916 | 5.5410 |
| 0.0001 | 0.0 | 0.001 | 0.4737 | 0.0237 | 0.1676 | 5.6293 |
| 0.0001 | 0.5 | 0.001 | 0.4707 | 0.0235 | 0.1667 | 5.6265 |
| 0.0001 | 0.2 | 0.001 | 0.4681 | 0.0234 | 0.1644 | 5.6404 |
| 1e-06 | 0.0 | 0.0001 | 0.4460 | 0.0223 | 0.1569 | 5.7470 |
| 1e-06 | 0.5 | 0.0001 | 0.4473 | 0.0224 | 0.1575 | 5.7527 |
| 1e-06 | 0.2 | 0.0001 | 0.4455 | 0.0223 | 0.1573 | 5.7503 |
| 1e-05 | 0.0 | 0.0001 | 0.4422 | 0.0221 | 0.1534 | 5.7087 |
| 1e-05 | 0.5 | 0.0001 | 0.4430 | 0.0222 | 0.1529 | 5.7103 |
| 1e-05 | 0.2 | 0.0001 | 0.4402 | 0.0220 | 0.1525 | 5.7157 |
| 0.0001 | 0.0 | 0.0001 | 0.4154 | 0.0208 | 0.1343 | 5.8321 |
| 0.0001 | 0.2 | 0.0001 | 0.4139 | 0.0207 | 0.1355 | 5.8342 |
| 0.0001 | 0.5 | 0.0001 | 0.4126 | 0.0206 | 0.1337 | 5.8342 |
| 1e-06 | 0.0 | 1e-05 | 0.2620 | 0.0131 | 0.0791 | 6.9571 |
| 1e-05 | 0.2 | 1e-05 | 0.2606 | 0.0130 | 0.0773 | 6.9478 |
| 1e-06 | 0.2 | 1e-05 | 0.2588 | 0.0129 | 0.0791 | 6.9607 |
| 1e-05 | 0.0 | 1e-05 | 0.2573 | 0.0129 | 0.0760 | 6.9604 |
| 1e-06 | 0.5 | 1e-05 | 0.2565 | 0.0128 | 0.0764 | 6.9800 |
| 1e-05 | 0.5 | 1e-05 | 0.2559 | 0.0128 | 0.0755 | 6.9457 |
| 0.0001 | 0.2 | 1e-05 | 0.2525 | 0.0126 | 0.0739 | 7.0286 |
| 0.0001 | 0.5 | 1e-05 | 0.2506 | 0.0125 | 0.0738 | 7.0368 |
| 0.0001 | 0.0 | 1e-05 | 0.2487 | 0.0124 | 0.0744 | 7.0491 |-->


Sorted by MRR@20 (highest to lowest, top 5):
| weight_decay | dropout_rate | learning_rate | MRR@20 | R@20 | P@20 | Loss |
|-------------|--------------|---------------|---------|------|------|------|
| 1e-06 | 0.5 | 0.001 | 0.1943 | 0.4910 | 0.0246 | 5.7627 |
| 1e-06 | 0.0 | 0.001 | 0.1926 | 0.4904 | 0.0245 | 5.7478 |
| 1e-06 | 0.2 | 0.001 | 0.1926 | 0.4909 | 0.0245 | 5.7427 |
| 1e-05 | 0.5 | 0.001 | 0.1931 | 0.4960 | 0.0248 | 5.5442 |
| 1e-05 | 0.0 | 0.001 | 0.1916 | 0.4964 | 0.0248 | 5.5573 |
<!--| 1e-05 | 0.2 | 0.001 | 0.1916 | 0.4974 | 0.0249 | 5.5410 |
| 0.0001 | 0.0 | 0.001 | 0.1676 | 0.4737 | 0.0237 | 5.6293 |
| 0.0001 | 0.5 | 0.001 | 0.1667 | 0.4707 | 0.0235 | 5.6265 |
| 0.0001 | 0.2 | 0.001 | 0.1644 | 0.4681 | 0.0234 | 5.6404 |
| 1e-06 | 0.0 | 0.0001 | 0.1569 | 0.4460 | 0.0223 | 5.7470 |
| 1e-06 | 0.5 | 0.0001 | 0.1575 | 0.4473 | 0.0224 | 5.7527 |
| 1e-06 | 0.2 | 0.0001 | 0.1573 | 0.4455 | 0.0223 | 5.7503 |
| 1e-05 | 0.0 | 0.0001 | 0.1534 | 0.4422 | 0.0221 | 5.7087 |
| 1e-05 | 0.5 | 0.0001 | 0.1529 | 0.4430 | 0.0222 | 5.7103 |
| 1e-05 | 0.2 | 0.0001 | 0.1525 | 0.4402 | 0.0220 | 5.7157 |
| 0.0001 | 0.2 | 0.0001 | 0.1355 | 0.4139 | 0.0207 | 5.8342 |
| 0.0001 | 0.0 | 0.0001 | 0.1343 | 0.4154 | 0.0208 | 5.8321 |
| 0.0001 | 0.5 | 0.0001 | 0.1337 | 0.4126 | 0.0206 | 5.8342 |
| 1e-06 | 0.0 | 1e-05 | 0.0791 | 0.2620 | 0.0131 | 6.9571 |
| 1e-06 | 0.2 | 1e-05 | 0.0791 | 0.2588 | 0.0129 | 6.9607 |
| 1e-06 | 0.5 | 1e-05 | 0.0764 | 0.2565 | 0.0128 | 6.9800 |
| 1e-05 | 0.0 | 1e-05 | 0.0760 | 0.2573 | 0.0129 | 6.9604 |
| 1e-05 | 0.5 | 1e-05 | 0.0755 | 0.2559 | 0.0128 | 6.9457 |
| 1e-05 | 0.2 | 1e-05 | 0.0773 | 0.2606 | 0.0130 | 6.9478 |
| 0.0001 | 0.0 | 1e-05 | 0.0744 | 0.2487 | 0.0124 | 7.0491 |
| 0.0001 | 0.2 | 1e-05 | 0.0739 | 0.2525 | 0.0126 | 7.0286 |
| 0.0001 | 0.5 | 1e-05 | 0.0738 | 0.2506 | 0.0125 | 7.0368 |-->

Key findings:
* Best configuration for R@20:
    * weight_decay = 1e-05
    * dropout_rate = 0.2
    * learning_rate = 0.001
    * Achieves R@20 = 0.4974 and MRR@20 = 0.1916
* Best configuration for MRR@20:
    * weight_decay = 1e-06
    * dropout_rate = 0.5
    * learning_rate = 0.001
    * Achieves MRR@20 = 0.1943 and R@20 = 0.4910

Common patterns:
* Higher learning rate (0.001) consistently performs better
* Lower weight decay (1e-05 or 1e-06) yields better results
* Dropout rate impact is less significant when other parameters are optimal

The top configurations are very close in performance, with the main difference being the trade-off between slightly better recall or MRR.

## Graph With Embeddings and Attention

## Graph With Embeddings and Attentional Aggregation

Sorted by Recall@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | epoch | R@20 | P@20 | MRR@20 | Loss |
|-------------|--------------|---------------|-------|------|------|--------|------|
| 1e-06 | 0.0 | 0.001 | 4 | 0.6926050186157227 | 0.03463025018572807 | 0.35357096791267395 | 4.035938739776611 |
| 1e-06 | 0.2 | 0.001 | 4 | 0.6657100319862366 | 0.03328549861907959 | 0.33777883648872375 | 4.2228617668151855 |
| 1e-05 | 0.0 | 0.001 | 4 | 0.654295027256012 | 0.03271475061774254 | 0.33197617530822754 | 4.32584810256958 |
| 1e-05 | 0.2 | 0.001 | 4 | 0.6370574831962585 | 0.03185287490487099 | 0.3217466175556183 | 4.458751201629639 |
<!--| 1e-06 | 0.5 | 0.001 | 4 | 0.6225699782371521 | 0.031128499656915665 | 0.31738847494125366 | 4.541009426116943 |
| 1e-05 | 0.5 | 0.001 | 4 | 0.6067125201225281 | 0.030335625633597374 | 0.30699387192726135 | 4.692712306976318 |
| 0.0001 | 0.0 | 0.001 | 4 | 0.571940004825592 | 0.028597000986337662 | 0.2794182598590851 | 4.949954509735107 |
| 0.0001 | 0.2 | 0.001 | 4 | 0.5660175085067749 | 0.028300875797867775 | 0.27562835812568665 | 5.005668640136719 |
| 1e-06 | 0.0 | 0.0001 | 4 | 0.5642325282096863 | 0.028211625292897224 | 0.292792409658432 | 4.805997371673584 |
| 1e-06 | 0.2 | 0.0001 | 4 | 0.5566174983978271 | 0.027830874547362328 | 0.2887302041053772 | 4.872871398925781 |
| 0.0001 | 0.5 | 0.001 | 4 | 0.5542525053024292 | 0.0277126245200634 | 0.2682848572731018 | 5.122139930725098 |
| 1e-05 | 0.0 | 0.0001 | 4 | 0.5540500283241272 | 0.02770249918103218 | 0.2811014652252197 | 4.86821174621582 |
| 1e-06 | 0.5 | 0.0001 | 4 | 0.5394949913024902 | 0.0269747506827116 | 0.2790975868701935 | 5.037694454193115 |
| 1e-05 | 0.2 | 0.0001 | 4 | 0.5394924879074097 | 0.026974625885486603 | 0.27096155285835266 | 5.009651184082031 |
| 1e-05 | 0.5 | 0.0001 | 4 | 0.5327200293540955 | 0.026636000722646713 | 0.26735392212867737 | 5.074589252471924 |
| 0.0001 | 0.0 | 0.0001 | 4 | 0.5168949961662292 | 0.025844749063253403 | 0.23838874697685242 | 5.154637336730957 |
| 0.0001 | 0.2 | 0.0001 | 4 | 0.509784996509552 | 0.02548925019800663 | 0.23130708932876587 | 5.2104597091674805 |
| 0.0001 | 0.5 | 0.0001 | 4 | 0.49024251103401184 | 0.024512125179171562 | 0.2193153351545334 | 5.355796813964844 |
| 1e-05 | 0.0 | 1e-05 | 4 | 0.3297474980354309 | 0.016487374901771545 | 0.12825560569763184 | 6.461963176727295 |
| 1e-06 | 0.0 | 1e-05 | 4 | 0.3293974995613098 | 0.01646987535059452 | 0.1266413927078247 | 6.455257892608643 |
| 0.0001 | 0.0 | 1e-05 | 4 | 0.3216024935245514 | 0.01608012430369854 | 0.12460421770811081 | 6.576822280883789 |
| 1e-05 | 0.2 | 1e-05 | 4 | 0.31350499391555786 | 0.015675250440835953 | 0.11745910346508026 | 6.552011966705322 |
| 1e-06 | 0.2 | 1e-05 | 4 | 0.3128100037574768 | 0.0156405009329319 | 0.11615153402090073 | 6.543017864227295 |
| 0.0001 | 0.2 | 1e-05 | 4 | 0.3025600016117096 | 0.01512799970805645 | 0.11011788249015808 | 6.680130958557129 |
| 1e-05 | 0.5 | 1e-05 | 4 | 0.2875649929046631 | 0.014378249645233154 | 0.10194917023181915 | 6.732785701751709 |
| 1e-06 | 0.5 | 1e-05 | 4 | 0.28674250841140747 | 0.014337125234305859 | 0.1020359918475151 | 6.727509498596191 |
| 0.0001 | 0.5 | 1e-05 | 4 | 0.279182493686676 | 0.013959124684333801 | 0.09568469226360321 | 6.848642349243164 |-->

Sorted by MRR@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | epoch | MRR@20 | R@20 | P@20 | Loss |
|-------------|--------------|---------------|-------|--------|------|------|------|
| 1e-06 | 0.0 | 0.001 | 4 | 0.35357096791267395 | 0.6926050186157227 | 0.03463025018572807 | 4.035938739776611 |
| 1e-06 | 0.2 | 0.001 | 4 | 0.33777883648872375 | 0.6657100319862366 | 0.03328549861907959 | 4.2228617668151855 |
| 1e-05 | 0.0 | 0.001 | 4 | 0.33197617530822754 | 0.654295027256012 | 0.03271475061774254 | 4.32584810256958 |
| 1e-05 | 0.2 | 0.001 | 4 | 0.3217466175556183 | 0.6370574831962585 | 0.03185287490487099 | 4.458751201629639 |
| 1e-06 | 0.5 | 0.001 | 4 | 0.31738847494125366 | 0.6225699782371521 | 0.031128499656915665 | 4.541009426116943 |
<!--| 1e-05 | 0.5 | 0.001 | 4 | 0.30699387192726135 | 0.6067125201225281 | 0.030335625633597374 | 4.692712306976318 |
| 1e-06 | 0.0 | 0.0001 | 4 | 0.292792409658432 | 0.5642325282096863 | 0.028211625292897224 | 4.805997371673584 |
| 1e-06 | 0.2 | 0.0001 | 4 | 0.2887302041053772 | 0.5566174983978271 | 0.027830874547362328 | 4.872871398925781 |
| 1e-05 | 0.0 | 0.0001 | 4 | 0.2811014652252197 | 0.5540500283241272 | 0.02770249918103218 | 4.86821174621582 |
| 0.0001 | 0.0 | 0.001 | 4 | 0.2794182598590851 | 0.571940004825592 | 0.028597000986337662 | 4.949954509735107 |
| 1e-06 | 0.5 | 0.0001 | 4 | 0.2790975868701935 | 0.5394949913024902 | 0.0269747506827116 | 5.037694454193115 |
| 0.0001 | 0.2 | 0.001 | 4 | 0.27562835812568665 | 0.5660175085067749 | 0.028300875797867775 | 5.005668640136719 |
| 1e-05 | 0.2 | 0.0001 | 4 | 0.27096155285835266 | 0.5394924879074097 | 0.026974625885486603 | 5.009651184082031 |
| 0.0001 | 0.5 | 0.001 | 4 | 0.2682848572731018 | 0.5542525053024292 | 0.0277126245200634 | 5.122139930725098 |
| 1e-05 | 0.5 | 0.0001 | 4 | 0.26735392212867737 | 0.5327200293540955 | 0.026636000722646713 | 5.074589252471924 |
| 0.0001 | 0.0 | 0.0001 | 4 | 0.23838874697685242 | 0.5168949961662292 | 0.025844749063253403 | 5.154637336730957 |
| 0.0001 | 0.2 | 0.0001 | 4 | 0.23130708932876587 | 0.509784996509552 | 0.02548925019800663 | 5.2104597091674805 |
| 0.0001 | 0.5 | 0.0001 | 4 | 0.2193153351545334 | 0.49024251103401184 | 0.024512125179171562 | 5.355796813964844 |
| 1e-05 | 0.0 | 1e-05 | 4 | 0.12825560569763184 | 0.3297474980354309 | 0.016487374901771545 | 6.461963176727295 |
| 1e-06 | 0.0 | 1e-05 | 4 | 0.1266413927078247 | 0.3293974995613098 | 0.01646987535059452 | 6.455257892608643 |
| 0.0001 | 0.0 | 1e-05 | 4 | 0.12460421770811081 | 0.3216024935245514 | 0.01608012430369854 | 6.576822280883789 |
| 1e-05 | 0.2 | 1e-05 | 4 | 0.11745910346508026 | 0.31350499391555786 | 0.015675250440835953 | 6.552011966705322 |
| 1e-06 | 0.2 | 1e-05 | 4 | 0.11615153402090073 | 0.3128100037574768 | 0.0156405009329319 | 6.543017864227295 |
| 0.0001 | 0.2 | 1e-05 | 4 | 0.11011788249015808 | 0.3025600016117096 | 0.01512799970805645 | 6.680130958557129 |
| 1e-06 | 0.5 | 1e-05 | 4 | 0.1020359918475151 | 0.28674250841140747 | 0.014337125234305859 | 6.727509498596191 |
| 1e-05 | 0.5 | 1e-05 | 4 | 0.10194917023181915 | 0.2875649929046631 | 0.014378249645233154 | 6.732785701751709 |
| 0.0001 | 0.5 | 1e-05 | 4 | 0.09568469226360321 | 0.279182493686676 | 0.013959124684333801 | 6.848642349243164 |-->

## Key Findings:
* The best configuration for Recall@20:
  - weight_decay = 1e-06
  - dropout_rate = 0.0
  - learning_rate = 0.001
  - Achieves R@20 = 0.6926050186157227 and MRR@20 = 0.35357096791267395
* The best configuration for MRR@20:
  - weight_decay = 1e-06
  - dropout_rate = 0.0
  - learning_rate = 0.001
  - Achieves MRR@20 = 0.35357096791267395 and R@20 = 0.6926050186157227

## Common Patterns:
* Higher learning rate (0.001) consistently performs better
* Lower weight decay (1e-05 or 1e-06) yields better results
* Dropout rate impact is less significant when other parameters are optimal
