# Experiments report

## Graph With Embeddings

Sorted by Recall@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | R@20 | P@20 | MRR@20 | Loss |
|-------------|--------------|---------------|------|------|---------|------|
| 1e-05 | 0.0 | 0.001 | 0.4964 | 0.0248 | 0.1916 | 5.5573 |
| 1e-05 | 0.5 | 0.001 | 0.4960 | 0.0248 | 0.1931 | 5.5442 |
| 1e-06 | 0.5 | 0.001 | 0.4910 | 0.0246 | 0.1943 | 5.7627 |
| 1e-06 | 0.0 | 0.001 | 0.4904 | 0.0245 | 0.1926 | 5.7478 |
| 1e-06 | 0.2 | 0.001 | 0.4909 | 0.0245 | 0.1926 | 5.7427 |
<!--| 1e-05 | 0.2 | 0.001 | 0.4974 | 0.0249 | 0.1916 | 5.5410 |
| 0.0001 | 0.0 | 0.001 | 0.4737 | 0.0237 | 0.1676 | 5.6293 |
| 0.0001 | 0.5 | 0.001 | 0.4707 | 0.0235 | 0.1667 | 5.6265 |
| 0.0001 | 0.2 | 0.001 | 0.4681 | 0.0234 | 0.1644 | 5.6404 |
| 1e-06 | 0.0 | 0.0001 | 0.4460 | 0.0223 | 0.1569 | 5.7470 |
| 1e-06 | 0.5 | 0.0001 | 0.4473 | 0.0224 | 0.1575 | 5.7527 |
| 1e-06 | 0.2 | 0.0001 | 0.4455 | 0.0223 | 0.1573 | 5.7503 |
| 1e-05 | 0.0 | 0.0001 | 0.4422 | 0.0221 | 0.1534 | 5.7087 |
| 1e-05 | 0.5 | 0.0001 | 0.4430 | 0.0222 | 0.1529 | 5.7103 |
| 1e-05 | 0.2 | 0.0001 | 0.4402 | 0.0220 | 0.1525 | 5.7157 |
| 0.0001 | 0.0 | 0.0001 | 0.4154 | 0.0208 | 0.1343 | 5.8321 |
| 0.0001 | 0.2 | 0.0001 | 0.4139 | 0.0207 | 0.1355 | 5.8342 |
| 0.0001 | 0.5 | 0.0001 | 0.4126 | 0.0206 | 0.1337 | 5.8342 |
| 1e-06 | 0.0 | 1e-05 | 0.2620 | 0.0131 | 0.0791 | 6.9571 |
| 1e-05 | 0.2 | 1e-05 | 0.2606 | 0.0130 | 0.0773 | 6.9478 |
| 1e-06 | 0.2 | 1e-05 | 0.2588 | 0.0129 | 0.0791 | 6.9607 |
| 1e-05 | 0.0 | 1e-05 | 0.2573 | 0.0129 | 0.0760 | 6.9604 |
| 1e-06 | 0.5 | 1e-05 | 0.2565 | 0.0128 | 0.0764 | 6.9800 |
| 1e-05 | 0.5 | 1e-05 | 0.2559 | 0.0128 | 0.0755 | 6.9457 |
| 0.0001 | 0.2 | 1e-05 | 0.2525 | 0.0126 | 0.0739 | 7.0286 |
| 0.0001 | 0.5 | 1e-05 | 0.2506 | 0.0125 | 0.0738 | 7.0368 |
| 0.0001 | 0.0 | 1e-05 | 0.2487 | 0.0124 | 0.0744 | 7.0491 |-->


Sorted by MRR@20 (highest to lowest, top 5):
| weight_decay | dropout_rate | learning_rate | MRR@20 | R@20 | P@20 | Loss |
|-------------|--------------|---------------|---------|------|------|------|
| 1e-06 | 0.5 | 0.001 | 0.1943 | 0.4910 | 0.0246 | 5.7627 |
| 1e-06 | 0.0 | 0.001 | 0.1926 | 0.4904 | 0.0245 | 5.7478 |
| 1e-06 | 0.2 | 0.001 | 0.1926 | 0.4909 | 0.0245 | 5.7427 |
| 1e-05 | 0.5 | 0.001 | 0.1931 | 0.4960 | 0.0248 | 5.5442 |
| 1e-05 | 0.0 | 0.001 | 0.1916 | 0.4964 | 0.0248 | 5.5573 |
<!--| 1e-05 | 0.2 | 0.001 | 0.1916 | 0.4974 | 0.0249 | 5.5410 |
| 0.0001 | 0.0 | 0.001 | 0.1676 | 0.4737 | 0.0237 | 5.6293 |
| 0.0001 | 0.5 | 0.001 | 0.1667 | 0.4707 | 0.0235 | 5.6265 |
| 0.0001 | 0.2 | 0.001 | 0.1644 | 0.4681 | 0.0234 | 5.6404 |
| 1e-06 | 0.0 | 0.0001 | 0.1569 | 0.4460 | 0.0223 | 5.7470 |
| 1e-06 | 0.5 | 0.0001 | 0.1575 | 0.4473 | 0.0224 | 5.7527 |
| 1e-06 | 0.2 | 0.0001 | 0.1573 | 0.4455 | 0.0223 | 5.7503 |
| 1e-05 | 0.0 | 0.0001 | 0.1534 | 0.4422 | 0.0221 | 5.7087 |
| 1e-05 | 0.5 | 0.0001 | 0.1529 | 0.4430 | 0.0222 | 5.7103 |
| 1e-05 | 0.2 | 0.0001 | 0.1525 | 0.4402 | 0.0220 | 5.7157 |
| 0.0001 | 0.2 | 0.0001 | 0.1355 | 0.4139 | 0.0207 | 5.8342 |
| 0.0001 | 0.0 | 0.0001 | 0.1343 | 0.4154 | 0.0208 | 5.8321 |
| 0.0001 | 0.5 | 0.0001 | 0.1337 | 0.4126 | 0.0206 | 5.8342 |
| 1e-06 | 0.0 | 1e-05 | 0.0791 | 0.2620 | 0.0131 | 6.9571 |
| 1e-06 | 0.2 | 1e-05 | 0.0791 | 0.2588 | 0.0129 | 6.9607 |
| 1e-06 | 0.5 | 1e-05 | 0.0764 | 0.2565 | 0.0128 | 6.9800 |
| 1e-05 | 0.0 | 1e-05 | 0.0760 | 0.2573 | 0.0129 | 6.9604 |
| 1e-05 | 0.5 | 1e-05 | 0.0755 | 0.2559 | 0.0128 | 6.9457 |
| 1e-05 | 0.2 | 1e-05 | 0.0773 | 0.2606 | 0.0130 | 6.9478 |
| 0.0001 | 0.0 | 1e-05 | 0.0744 | 0.2487 | 0.0124 | 7.0491 |
| 0.0001 | 0.2 | 1e-05 | 0.0739 | 0.2525 | 0.0126 | 7.0286 |
| 0.0001 | 0.5 | 1e-05 | 0.0738 | 0.2506 | 0.0125 | 7.0368 |-->

Key findings:
* Best configuration for R@20:
    * weight_decay = 1e-05
    * dropout_rate = 0.2
    * learning_rate = 0.001
    * Achieves R@20 = 0.4974 and MRR@20 = 0.1916
* Best configuration for MRR@20:
    * weight_decay = 1e-06
    * dropout_rate = 0.5
    * learning_rate = 0.001
    * Achieves MRR@20 = 0.1943 and R@20 = 0.4910

Common patterns:
* Higher learning rate (0.001) consistently performs better
* Lower weight decay (1e-05 or 1e-06) yields better results
* Dropout rate impact is less significant when other parameters are optimal

The top configurations are very close in performance, with the main difference being the trade-off between slightly better recall or MRR.

## Graph With Embeddings and Attention

## Graph with Embeddings and Attentional Aggregation
