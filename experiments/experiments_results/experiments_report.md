# Experiments report

## Graph With Embeddings

Sorted by Recall@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | R@20 | P@20 | MRR@20 | Loss |
|-------------|--------------|---------------|------|------|---------|------|
| 1e-05 | 0.0 | 0.001 | 0.4964 | 0.0248 | 0.1916 | 5.5573 |
| 1e-05 | 0.5 | 0.001 | 0.4960 | 0.0248 | 0.1931 | 5.5442 |
| 1e-06 | 0.5 | 0.001 | 0.4910 | 0.0246 | 0.1943 | 5.7627 |
| 1e-06 | 0.0 | 0.001 | 0.4904 | 0.0245 | 0.1926 | 5.7478 |
| 1e-06 | 0.2 | 0.001 | 0.4909 | 0.0245 | 0.1926 | 5.7427 |
<!--| 1e-05 | 0.2 | 0.001 | 0.4974 | 0.0249 | 0.1916 | 5.5410 |
| 0.0001 | 0.0 | 0.001 | 0.4737 | 0.0237 | 0.1676 | 5.6293 |
| 0.0001 | 0.5 | 0.001 | 0.4707 | 0.0235 | 0.1667 | 5.6265 |
| 0.0001 | 0.2 | 0.001 | 0.4681 | 0.0234 | 0.1644 | 5.6404 |
| 1e-06 | 0.0 | 0.0001 | 0.4460 | 0.0223 | 0.1569 | 5.7470 |
| 1e-06 | 0.5 | 0.0001 | 0.4473 | 0.0224 | 0.1575 | 5.7527 |
| 1e-06 | 0.2 | 0.0001 | 0.4455 | 0.0223 | 0.1573 | 5.7503 |
| 1e-05 | 0.0 | 0.0001 | 0.4422 | 0.0221 | 0.1534 | 5.7087 |
| 1e-05 | 0.5 | 0.0001 | 0.4430 | 0.0222 | 0.1529 | 5.7103 |
| 1e-05 | 0.2 | 0.0001 | 0.4402 | 0.0220 | 0.1525 | 5.7157 |
| 0.0001 | 0.0 | 0.0001 | 0.4154 | 0.0208 | 0.1343 | 5.8321 |
| 0.0001 | 0.2 | 0.0001 | 0.4139 | 0.0207 | 0.1355 | 5.8342 |
| 0.0001 | 0.5 | 0.0001 | 0.4126 | 0.0206 | 0.1337 | 5.8342 |
| 1e-06 | 0.0 | 1e-05 | 0.2620 | 0.0131 | 0.0791 | 6.9571 |
| 1e-05 | 0.2 | 1e-05 | 0.2606 | 0.0130 | 0.0773 | 6.9478 |
| 1e-06 | 0.2 | 1e-05 | 0.2588 | 0.0129 | 0.0791 | 6.9607 |
| 1e-05 | 0.0 | 1e-05 | 0.2573 | 0.0129 | 0.0760 | 6.9604 |
| 1e-06 | 0.5 | 1e-05 | 0.2565 | 0.0128 | 0.0764 | 6.9800 |
| 1e-05 | 0.5 | 1e-05 | 0.2559 | 0.0128 | 0.0755 | 6.9457 |
| 0.0001 | 0.2 | 1e-05 | 0.2525 | 0.0126 | 0.0739 | 7.0286 |
| 0.0001 | 0.5 | 1e-05 | 0.2506 | 0.0125 | 0.0738 | 7.0368 |
| 0.0001 | 0.0 | 1e-05 | 0.2487 | 0.0124 | 0.0744 | 7.0491 |-->


Sorted by MRR@20 (highest to lowest, top 5):
| weight_decay | dropout_rate | learning_rate | MRR@20 | R@20 | P@20 | Loss |
|-------------|--------------|---------------|---------|------|------|------|
| 1e-06 | 0.5 | 0.001 | 0.1943 | 0.4910 | 0.0246 | 5.7627 |
| 1e-06 | 0.0 | 0.001 | 0.1926 | 0.4904 | 0.0245 | 5.7478 |
| 1e-06 | 0.2 | 0.001 | 0.1926 | 0.4909 | 0.0245 | 5.7427 |
| 1e-05 | 0.5 | 0.001 | 0.1931 | 0.4960 | 0.0248 | 5.5442 |
| 1e-05 | 0.0 | 0.001 | 0.1916 | 0.4964 | 0.0248 | 5.5573 |
<!--| 1e-05 | 0.2 | 0.001 | 0.1916 | 0.4974 | 0.0249 | 5.5410 |
| 0.0001 | 0.0 | 0.001 | 0.1676 | 0.4737 | 0.0237 | 5.6293 |
| 0.0001 | 0.5 | 0.001 | 0.1667 | 0.4707 | 0.0235 | 5.6265 |
| 0.0001 | 0.2 | 0.001 | 0.1644 | 0.4681 | 0.0234 | 5.6404 |
| 1e-06 | 0.0 | 0.0001 | 0.1569 | 0.4460 | 0.0223 | 5.7470 |
| 1e-06 | 0.5 | 0.0001 | 0.1575 | 0.4473 | 0.0224 | 5.7527 |
| 1e-06 | 0.2 | 0.0001 | 0.1573 | 0.4455 | 0.0223 | 5.7503 |
| 1e-05 | 0.0 | 0.0001 | 0.1534 | 0.4422 | 0.0221 | 5.7087 |
| 1e-05 | 0.5 | 0.0001 | 0.1529 | 0.4430 | 0.0222 | 5.7103 |
| 1e-05 | 0.2 | 0.0001 | 0.1525 | 0.4402 | 0.0220 | 5.7157 |
| 0.0001 | 0.2 | 0.0001 | 0.1355 | 0.4139 | 0.0207 | 5.8342 |
| 0.0001 | 0.0 | 0.0001 | 0.1343 | 0.4154 | 0.0208 | 5.8321 |
| 0.0001 | 0.5 | 0.0001 | 0.1337 | 0.4126 | 0.0206 | 5.8342 |
| 1e-06 | 0.0 | 1e-05 | 0.0791 | 0.2620 | 0.0131 | 6.9571 |
| 1e-06 | 0.2 | 1e-05 | 0.0791 | 0.2588 | 0.0129 | 6.9607 |
| 1e-06 | 0.5 | 1e-05 | 0.0764 | 0.2565 | 0.0128 | 6.9800 |
| 1e-05 | 0.0 | 1e-05 | 0.0760 | 0.2573 | 0.0129 | 6.9604 |
| 1e-05 | 0.5 | 1e-05 | 0.0755 | 0.2559 | 0.0128 | 6.9457 |
| 1e-05 | 0.2 | 1e-05 | 0.0773 | 0.2606 | 0.0130 | 6.9478 |
| 0.0001 | 0.0 | 1e-05 | 0.0744 | 0.2487 | 0.0124 | 7.0491 |
| 0.0001 | 0.2 | 1e-05 | 0.0739 | 0.2525 | 0.0126 | 7.0286 |
| 0.0001 | 0.5 | 1e-05 | 0.0738 | 0.2506 | 0.0125 | 7.0368 |-->

Key findings:
* Best configuration for R@20:
    * weight_decay = 1e-05
    * dropout_rate = 0.2
    * learning_rate = 0.001
    * Achieves R@20 = 0.4974 and MRR@20 = 0.1916
* Best configuration for MRR@20:
    * weight_decay = 1e-06
    * dropout_rate = 0.5
    * learning_rate = 0.001
    * Achieves MRR@20 = 0.1943 and R@20 = 0.4910

Common patterns:
* Higher learning rate (0.001) consistently performs better
* Lower weight decay (1e-05 or 1e-06) yields better results
* Dropout rate impact is less significant when other parameters are optimal

The top configurations are very close in performance, with the main difference being the trade-off between slightly better recall or MRR.

## Graph With Embeddings and Attention

Sorted by Recall@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | epoch | R@20 | P@20 | MRR@20 | Loss |
|-------------|--------------|---------------|-------|------|------|--------|------|
| 1e-06 | 0.5 | 0.001 | 4 | 0.6393100023269653 | 0.031965501606464386 | 0.3014877736568451 | 4.3797831535339355 |
| 1e-06 | 0.0 | 0.001 | 4 | 0.6391950249671936 | 0.03195974975824356 | 0.3013673424720764 | 4.382649898529053 |
| 1e-06 | 0.2 | 0.001 | 4 | 0.6384750008583069 | 0.031923748552799225 | 0.30093035101890564 | 4.382964611053467 |
| 1e-05 | 0.5 | 0.001 | 4 | 0.6148974895477295 | 0.030744874849915504 | 0.29349759221076965 | 4.608500957489014 |
| 1e-05 | 0.2 | 0.001 | 4 | 0.614127516746521 | 0.03070637583732605 | 0.29328107833862305 | 4.611515998840332 |
<!--| 1e-05 | 0.0 | 0.001 | 4 | 0.613937497138977 | 0.030696874484419823 | 0.2929316759109497 | 4.611868858337402 |
| 0.0001 | 0.0 | 0.001 | 4 | 0.5577549934387207 | 0.027887750416994095 | 0.2612643837928772 | 5.07012939453125 |
| 0.0001 | 0.2 | 0.001 | 4 | 0.5576549768447876 | 0.02788274921476841 | 0.2622734010219574 | 5.0680413246154785 |
| 0.0001 | 0.5 | 0.001 | 4 | 0.5563399791717529 | 0.027816999703645706 | 0.2615034580230713 | 5.072461128234863 |
| 1e-06 | 0.0 | 0.0001 | 4 | 0.501307487487793 | 0.025065375491976738 | 0.2042996734380722 | 5.21246337890625 |
| 1e-05 | 0.2 | 0.0001 | 4 | 0.49881500005722046 | 0.024940749630331993 | 0.20068959891796112 | 5.226296901702881 |
| 1e-06 | 0.5 | 0.0001 | 4 | 0.4984700083732605 | 0.024923499673604965 | 0.20247051119804382 | 5.227555274963379 |
| 1e-06 | 0.2 | 0.0001 | 4 | 0.49764248728752136 | 0.024882124736905098 | 0.20047928392887115 | 5.234585285186768 |
| 1e-05 | 0.0 | 0.0001 | 4 | 0.4969624876976013 | 0.024848125874996185 | 0.19949322938919067 | 5.235161781311035 |
| 1e-05 | 0.5 | 0.0001 | 4 | 0.49663498997688293 | 0.024831749498844147 | 0.19848744571208954 | 5.246513366699219 |
| 0.0001 | 0.0 | 0.0001 | 4 | 0.4795899987220764 | 0.02397949993610382 | 0.18097154796123505 | 5.413155555725098 |
| 0.0001 | 0.2 | 0.0001 | 4 | 0.4791100025177002 | 0.02395549975335598 | 0.17911159992218018 | 5.410048484802246 |
| 0.0001 | 0.5 | 0.0001 | 4 | 0.47766751050949097 | 0.02388337440788746 | 0.1792837679386139 | 5.418819904327393 |
| 1e-05 | 0.5 | 1e-05 | 4 | 0.2562449872493744 | 0.012812250293791294 | 0.0748150497674942 | 6.867353439331055 |
| 1e-05 | 0.0 | 1e-05 | 4 | 0.2530674934387207 | 0.012653375044465065 | 0.07448416948318481 | 6.8933305740356445 |
| 1e-06 | 0.0 | 1e-05 | 4 | 0.25184500217437744 | 0.012592249549925327 | 0.07485687732696533 | 6.879955768585205 |
| 1e-05 | 0.2 | 1e-05 | 4 | 0.2516374886035919 | 0.012581874616444111 | 0.07253377139568329 | 6.911962509155273 |
| 1e-06 | 0.2 | 1e-05 | 4 | 0.25087499618530273 | 0.012543749995529652 | 0.07368679344654083 | 6.879542350769043 |
| 1e-06 | 0.5 | 1e-05 | 4 | 0.24903249740600586 | 0.012451625429093838 | 0.07310476899147034 | 6.912110328674316 |
| 0.0001 | 0.2 | 1e-05 | 4 | 0.24730749428272247 | 0.012365374714136124 | 0.07339473068714142 | 7.007232666015625 |
| 0.0001 | 0.5 | 1e-05 | 4 | 0.24476000666618347 | 0.012237999588251114 | 0.07210428267717361 | 7.030819416046143 |
| 0.0001 | 0.0 | 1e-05 | 4 | 0.2444400042295456 | 0.012222000397741795 | 0.07109437882900238 | 7.037554740905762 |-->

Sorted by MRR@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | epoch | MRR@20 | R@20 | P@20 | Loss |
|-------------|--------------|---------------|-------|--------|------|------|------|
| 1e-06 | 0.5 | 0.001 | 4 | 0.3014877736568451 | 0.6393100023269653 | 0.031965501606464386 | 4.3797831535339355 |
| 1e-06 | 0.0 | 0.001 | 4 | 0.3013673424720764 | 0.6391950249671936 | 0.03195974975824356 | 4.382649898529053 |
| 1e-06 | 0.2 | 0.001 | 4 | 0.30093035101890564 | 0.6384750008583069 | 0.031923748552799225 | 4.382964611053467 |
| 1e-05 | 0.5 | 0.001 | 4 | 0.29349759221076965 | 0.6148974895477295 | 0.030744874849915504 | 4.608500957489014 |
| 1e-05 | 0.2 | 0.001 | 4 | 0.29328107833862305 | 0.614127516746521 | 0.03070637583732605 | 4.611515998840332 |
<!--| 1e-05 | 0.0 | 0.001 | 4 | 0.2929316759109497 | 0.613937497138977 | 0.030696874484419823 | 4.611868858337402 |
| 0.0001 | 0.2 | 0.001 | 4 | 0.2622734010219574 | 0.5576549768447876 | 0.02788274921476841 | 5.0680413246154785 |
| 0.0001 | 0.5 | 0.001 | 4 | 0.2615034580230713 | 0.5563399791717529 | 0.027816999703645706 | 5.072461128234863 |
| 0.0001 | 0.0 | 0.001 | 4 | 0.2612643837928772 | 0.5577549934387207 | 0.027887750416994095 | 5.07012939453125 |
| 1e-06 | 0.0 | 0.0001 | 4 | 0.2042996734380722 | 0.501307487487793 | 0.025065375491976738 | 5.21246337890625 |
| 1e-06 | 0.5 | 0.0001 | 4 | 0.20247051119804382 | 0.4984700083732605 | 0.024923499673604965 | 5.227555274963379 |
| 1e-05 | 0.2 | 0.0001 | 4 | 0.20068959891796112 | 0.49881500005722046 | 0.024940749630331993 | 5.226296901702881 |
| 1e-06 | 0.2 | 0.0001 | 4 | 0.20047928392887115 | 0.49764248728752136 | 0.024882124736905098 | 5.234585285186768 |
| 1e-05 | 0.0 | 0.0001 | 4 | 0.19949322938919067 | 0.4969624876976013 | 0.024848125874996185 | 5.235161781311035 |
| 1e-05 | 0.5 | 0.0001 | 4 | 0.19848744571208954 | 0.49663498997688293 | 0.024831749498844147 | 5.246513366699219 |
| 0.0001 | 0.0 | 0.0001 | 4 | 0.18097154796123505 | 0.4795899987220764 | 0.02397949993610382 | 5.413155555725098 |
| 0.0001 | 0.5 | 0.0001 | 4 | 0.1792837679386139 | 0.47766751050949097 | 0.02388337440788746 | 5.418819904327393 |
| 0.0001 | 0.2 | 0.0001 | 4 | 0.17911159992218018 | 0.4791100025177002 | 0.02395549975335598 | 5.410048484802246 |
| 1e-06 | 0.0 | 1e-05 | 4 | 0.07485687732696533 | 0.25184500217437744 | 0.012592249549925327 | 6.879955768585205 |
| 1e-05 | 0.5 | 1e-05 | 4 | 0.0748150497674942 | 0.2562449872493744 | 0.012812250293791294 | 6.867353439331055 |
| 1e-05 | 0.0 | 1e-05 | 4 | 0.07448416948318481 | 0.2530674934387207 | 0.012653375044465065 | 6.8933305740356445 |
| 1e-06 | 0.2 | 1e-05 | 4 | 0.07368679344654083 | 0.25087499618530273 | 0.012543749995529652 | 6.879542350769043 |
| 0.0001 | 0.2 | 1e-05 | 4 | 0.07339473068714142 | 0.24730749428272247 | 0.012365374714136124 | 7.007232666015625 |
| 1e-06 | 0.5 | 1e-05 | 4 | 0.07310476899147034 | 0.24903249740600586 | 0.012451625429093838 | 6.912110328674316 |
| 1e-05 | 0.2 | 1e-05 | 4 | 0.07253377139568329 | 0.2516374886035919 | 0.012581874616444111 | 6.911962509155273 |
| 0.0001 | 0.5 | 1e-05 | 4 | 0.07210428267717361 | 0.24476000666618347 | 0.012237999588251114 | 7.030819416046143 |
| 0.0001 | 0.0 | 1e-05 | 4 | 0.07109437882900238 | 0.2444400042295456 | 0.012222000397741795 | 7.037554740905762 |-->

## Key Findings:
* The best configuration for Recall@20:
  - weight_decay = 1e-06
  - dropout_rate = 0.5
  - learning_rate = 0.001
  - Achieves R@20 = 0.6393100023269653 and MRR@20 = 0.3014877736568451
* The best configuration for MRR@20:
  - weight_decay = 1e-06
  - dropout_rate = 0.5
  - learning_rate = 0.001
  - Achieves MRR@20 = 0.3014877736568451 and R@20 = 0.6393100023269653

## Common Patterns:
* Higher learning rate (0.001) consistently performs better
* Lower weight decay (1e-05 or 1e-06) yields better results
* Dropout rate impact is less significant when other parameters are optimal

## Graph With Embeddings and Attentional Aggregation

Sorted by Recall@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | epoch | R@20 | P@20 | MRR@20 | Loss |
|-------------|--------------|---------------|-------|------|------|--------|------|
| 1e-06 | 0.0 | 0.001 | 4 | 0.6926050186157227 | 0.03463025018572807 | 0.35357096791267395 | 4.035938739776611 |
| 1e-06 | 0.2 | 0.001 | 4 | 0.6657100319862366 | 0.03328549861907959 | 0.33777883648872375 | 4.2228617668151855 |
| 1e-05 | 0.0 | 0.001 | 4 | 0.654295027256012 | 0.03271475061774254 | 0.33197617530822754 | 4.32584810256958 |
| 1e-05 | 0.2 | 0.001 | 4 | 0.6370574831962585 | 0.03185287490487099 | 0.3217466175556183 | 4.458751201629639 |
<!--| 1e-06 | 0.5 | 0.001 | 4 | 0.6225699782371521 | 0.031128499656915665 | 0.31738847494125366 | 4.541009426116943 |
| 1e-05 | 0.5 | 0.001 | 4 | 0.6067125201225281 | 0.030335625633597374 | 0.30699387192726135 | 4.692712306976318 |
| 0.0001 | 0.0 | 0.001 | 4 | 0.571940004825592 | 0.028597000986337662 | 0.2794182598590851 | 4.949954509735107 |
| 0.0001 | 0.2 | 0.001 | 4 | 0.5660175085067749 | 0.028300875797867775 | 0.27562835812568665 | 5.005668640136719 |
| 1e-06 | 0.0 | 0.0001 | 4 | 0.5642325282096863 | 0.028211625292897224 | 0.292792409658432 | 4.805997371673584 |
| 1e-06 | 0.2 | 0.0001 | 4 | 0.5566174983978271 | 0.027830874547362328 | 0.2887302041053772 | 4.872871398925781 |
| 0.0001 | 0.5 | 0.001 | 4 | 0.5542525053024292 | 0.0277126245200634 | 0.2682848572731018 | 5.122139930725098 |
| 1e-05 | 0.0 | 0.0001 | 4 | 0.5540500283241272 | 0.02770249918103218 | 0.2811014652252197 | 4.86821174621582 |
| 1e-06 | 0.5 | 0.0001 | 4 | 0.5394949913024902 | 0.0269747506827116 | 0.2790975868701935 | 5.037694454193115 |
| 1e-05 | 0.2 | 0.0001 | 4 | 0.5394924879074097 | 0.026974625885486603 | 0.27096155285835266 | 5.009651184082031 |
| 1e-05 | 0.5 | 0.0001 | 4 | 0.5327200293540955 | 0.026636000722646713 | 0.26735392212867737 | 5.074589252471924 |
| 0.0001 | 0.0 | 0.0001 | 4 | 0.5168949961662292 | 0.025844749063253403 | 0.23838874697685242 | 5.154637336730957 |
| 0.0001 | 0.2 | 0.0001 | 4 | 0.509784996509552 | 0.02548925019800663 | 0.23130708932876587 | 5.2104597091674805 |
| 0.0001 | 0.5 | 0.0001 | 4 | 0.49024251103401184 | 0.024512125179171562 | 0.2193153351545334 | 5.355796813964844 |
| 1e-05 | 0.0 | 1e-05 | 4 | 0.3297474980354309 | 0.016487374901771545 | 0.12825560569763184 | 6.461963176727295 |
| 1e-06 | 0.0 | 1e-05 | 4 | 0.3293974995613098 | 0.01646987535059452 | 0.1266413927078247 | 6.455257892608643 |
| 0.0001 | 0.0 | 1e-05 | 4 | 0.3216024935245514 | 0.01608012430369854 | 0.12460421770811081 | 6.576822280883789 |
| 1e-05 | 0.2 | 1e-05 | 4 | 0.31350499391555786 | 0.015675250440835953 | 0.11745910346508026 | 6.552011966705322 |
| 1e-06 | 0.2 | 1e-05 | 4 | 0.3128100037574768 | 0.0156405009329319 | 0.11615153402090073 | 6.543017864227295 |
| 0.0001 | 0.2 | 1e-05 | 4 | 0.3025600016117096 | 0.01512799970805645 | 0.11011788249015808 | 6.680130958557129 |
| 1e-05 | 0.5 | 1e-05 | 4 | 0.2875649929046631 | 0.014378249645233154 | 0.10194917023181915 | 6.732785701751709 |
| 1e-06 | 0.5 | 1e-05 | 4 | 0.28674250841140747 | 0.014337125234305859 | 0.1020359918475151 | 6.727509498596191 |
| 0.0001 | 0.5 | 1e-05 | 4 | 0.279182493686676 | 0.013959124684333801 | 0.09568469226360321 | 6.848642349243164 |-->

Sorted by MRR@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | epoch | MRR@20 | R@20 | P@20 | Loss |
|-------------|--------------|---------------|-------|--------|------|------|------|
| 1e-06 | 0.0 | 0.001 | 4 | 0.35357096791267395 | 0.6926050186157227 | 0.03463025018572807 | 4.035938739776611 |
| 1e-06 | 0.2 | 0.001 | 4 | 0.33777883648872375 | 0.6657100319862366 | 0.03328549861907959 | 4.2228617668151855 |
| 1e-05 | 0.0 | 0.001 | 4 | 0.33197617530822754 | 0.654295027256012 | 0.03271475061774254 | 4.32584810256958 |
| 1e-05 | 0.2 | 0.001 | 4 | 0.3217466175556183 | 0.6370574831962585 | 0.03185287490487099 | 4.458751201629639 |
| 1e-06 | 0.5 | 0.001 | 4 | 0.31738847494125366 | 0.6225699782371521 | 0.031128499656915665 | 4.541009426116943 |
<!--| 1e-05 | 0.5 | 0.001 | 4 | 0.30699387192726135 | 0.6067125201225281 | 0.030335625633597374 | 4.692712306976318 |
| 1e-06 | 0.0 | 0.0001 | 4 | 0.292792409658432 | 0.5642325282096863 | 0.028211625292897224 | 4.805997371673584 |
| 1e-06 | 0.2 | 0.0001 | 4 | 0.2887302041053772 | 0.5566174983978271 | 0.027830874547362328 | 4.872871398925781 |
| 1e-05 | 0.0 | 0.0001 | 4 | 0.2811014652252197 | 0.5540500283241272 | 0.02770249918103218 | 4.86821174621582 |
| 0.0001 | 0.0 | 0.001 | 4 | 0.2794182598590851 | 0.571940004825592 | 0.028597000986337662 | 4.949954509735107 |
| 1e-06 | 0.5 | 0.0001 | 4 | 0.2790975868701935 | 0.5394949913024902 | 0.0269747506827116 | 5.037694454193115 |
| 0.0001 | 0.2 | 0.001 | 4 | 0.27562835812568665 | 0.5660175085067749 | 0.028300875797867775 | 5.005668640136719 |
| 1e-05 | 0.2 | 0.0001 | 4 | 0.27096155285835266 | 0.5394924879074097 | 0.026974625885486603 | 5.009651184082031 |
| 0.0001 | 0.5 | 0.001 | 4 | 0.2682848572731018 | 0.5542525053024292 | 0.0277126245200634 | 5.122139930725098 |
| 1e-05 | 0.5 | 0.0001 | 4 | 0.26735392212867737 | 0.5327200293540955 | 0.026636000722646713 | 5.074589252471924 |
| 0.0001 | 0.0 | 0.0001 | 4 | 0.23838874697685242 | 0.5168949961662292 | 0.025844749063253403 | 5.154637336730957 |
| 0.0001 | 0.2 | 0.0001 | 4 | 0.23130708932876587 | 0.509784996509552 | 0.02548925019800663 | 5.2104597091674805 |
| 0.0001 | 0.5 | 0.0001 | 4 | 0.2193153351545334 | 0.49024251103401184 | 0.024512125179171562 | 5.355796813964844 |
| 1e-05 | 0.0 | 1e-05 | 4 | 0.12825560569763184 | 0.3297474980354309 | 0.016487374901771545 | 6.461963176727295 |
| 1e-06 | 0.0 | 1e-05 | 4 | 0.1266413927078247 | 0.3293974995613098 | 0.01646987535059452 | 6.455257892608643 |
| 0.0001 | 0.0 | 1e-05 | 4 | 0.12460421770811081 | 0.3216024935245514 | 0.01608012430369854 | 6.576822280883789 |
| 1e-05 | 0.2 | 1e-05 | 4 | 0.11745910346508026 | 0.31350499391555786 | 0.015675250440835953 | 6.552011966705322 |
| 1e-06 | 0.2 | 1e-05 | 4 | 0.11615153402090073 | 0.3128100037574768 | 0.0156405009329319 | 6.543017864227295 |
| 0.0001 | 0.2 | 1e-05 | 4 | 0.11011788249015808 | 0.3025600016117096 | 0.01512799970805645 | 6.680130958557129 |
| 1e-06 | 0.5 | 1e-05 | 4 | 0.1020359918475151 | 0.28674250841140747 | 0.014337125234305859 | 6.727509498596191 |
| 1e-05 | 0.5 | 1e-05 | 4 | 0.10194917023181915 | 0.2875649929046631 | 0.014378249645233154 | 6.732785701751709 |
| 0.0001 | 0.5 | 1e-05 | 4 | 0.09568469226360321 | 0.279182493686676 | 0.013959124684333801 | 6.848642349243164 |-->

## Key Findings:
* The best configuration for Recall@20:
  - weight_decay = 1e-06
  - dropout_rate = 0.0
  - learning_rate = 0.001
  - Achieves R@20 = 0.6926050186157227 and MRR@20 = 0.35357096791267395
* The best configuration for MRR@20:
  - weight_decay = 1e-06
  - dropout_rate = 0.0
  - learning_rate = 0.001
  - Achieves MRR@20 = 0.35357096791267395 and R@20 = 0.6926050186157227

## Common Patterns:
* Higher learning rate (0.001) consistently performs better
* Lower weight decay (1e-05 or 1e-06) yields better results
* Dropout rate impact is less significant when other parameters are optimal
