# Experiments report

## Graph With Embeddings

Sorted by Recall@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | epoch | R@20 | MRR@20 | Loss |
|-------------|--------------|---------------|-------|------|--------|------|
| 1e-06 | 0.2 | 0.001 | 4 | 0.5052400231361389 | 0.19707946479320526 | 5.559047698974609 |
| 1e-05 | 0.2 | 0.001 | 4 | 0.5018799901008606 | 0.1922193169593811 | 5.502201080322266 |
| 1e-06 | 0.5 | 0.001 | 4 | 0.4990600049495697 | 0.18935446441173553 | 5.580509662628174 |
| 1e-05 | 0.0 | 0.001 | 4 | 0.4980199933052063 | 0.1920752227306366 | 5.553403854370117 |
| 1e-06 | 0.0 | 0.001 | 4 | 0.4921799898147583 | 0.19180966913700104 | 5.739207744598389 |
<!--| 1e-05 | 0.5 | 0.001 | 4 | 0.491239994764328 | 0.18112924695014954 | 5.604668617248535 |
| 0.0001 | 0.0 | 0.001 | 4 | 0.4700999855995178 | 0.16656771302223206 | 5.633676528930664 |
| 0.0001 | 0.2 | 0.001 | 4 | 0.46549999713897705 | 0.160623699426651 | 5.683471202850342 |
| 1e-06 | 0.0 | 0.0001 | 4 | 0.447299987077713 | 0.15808594226837158 | 5.748147010803223 |
| 0.0001 | 0.5 | 0.001 | 4 | 0.4466399848461151 | 0.14642344415187836 | 5.846370697021484 |
| 1e-05 | 0.2 | 0.0001 | 4 | 0.4447399973869324 | 0.15495237708091736 | 5.746347427368164 |
| 1e-05 | 0.0 | 0.0001 | 4 | 0.444599986076355 | 0.15416443347930908 | 5.699609756469727 |
| 1e-06 | 0.2 | 0.0001 | 4 | 0.44394001364707947 | 0.1569720059633255 | 5.787014961242676 |
| 1e-05 | 0.5 | 0.0001 | 4 | 0.4240800142288208 | 0.14460010826587677 | 5.9642109870910645 |
| 1e-06 | 0.5 | 0.0001 | 4 | 0.4229600131511688 | 0.14327310025691986 | 6.001766681671143 |
| 0.0001 | 0.0 | 0.0001 | 4 | 0.41255998611450195 | 0.13507984578609467 | 5.8413872718811035 |
| 0.0001 | 0.2 | 0.0001 | 4 | 0.412339985370636 | 0.13298428058624268 | 5.88568115234375 |
| 0.0001 | 0.5 | 0.0001 | 4 | 0.39706000685691833 | 0.1264013797044754 | 6.085857391357422 |
| 1e-05 | 0.0 | 1e-05 | 4 | 0.26151999831199646 | 0.07940536737442017 | 6.94926118850708 |
| 1e-06 | 0.0 | 1e-05 | 4 | 0.25982001423835754 | 0.0773734599351883 | 6.9737725257873535 |
| 1e-05 | 0.2 | 1e-05 | 4 | 0.2531000077724457 | 0.0767027884721756 | 7.007055759429932 |
| 0.0001 | 0.0 | 1e-05 | 4 | 0.2513599991798401 | 0.0765145868062973 | 7.026798248291016 |
| 1e-06 | 0.2 | 1e-05 | 4 | 0.2487799972295761 | 0.07485027611255646 | 7.044581413269043 |
| 0.0001 | 0.2 | 1e-05 | 4 | 0.24617999792099 | 0.07289301604032516 | 7.121115684509277 |
| 1e-05 | 0.5 | 1e-05 | 4 | 0.23270000517368317 | 0.06913117319345474 | 7.222418785095215 |
| 1e-06 | 0.5 | 1e-05 | 4 | 0.23136000335216522 | 0.06849869340658188 | 7.259231090545654 |
| 0.0001 | 0.5 | 1e-05 | 4 | 0.22910000383853912 | 0.06784408539533615 | 7.360254764556885 |-->


Sorted by MRR@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | epoch | MRR@20 | R@20 | Loss |
|-------------|--------------|---------------|-------|--------|------|------|
| 1e-06 | 0.2 | 0.001 | 4 | 0.19707946479320526 | 0.5052400231361389 | 5.559047698974609 |
| 1e-05 | 0.2 | 0.001 | 4 | 0.1922193169593811 | 0.5018799901008606 | 5.502201080322266 |
| 1e-05 | 0.0 | 0.001 | 4 | 0.1920752227306366 | 0.4980199933052063 | 5.553403854370117 |
| 1e-06 | 0.0 | 0.001 | 4 | 0.19180966913700104 | 0.4921799898147583 | 5.739207744598389 |
| 1e-06 | 0.5 | 0.001 | 4 | 0.18935446441173553 | 0.4990600049495697 | 5.580509662628174 |
<!--| 1e-05 | 0.5 | 0.001 | 4 | 0.18112924695014954 | 0.491239994764328 | 5.604668617248535 |
| 0.0001 | 0.0 | 0.001 | 4 | 0.16656771302223206 | 0.4700999855995178 | 5.633676528930664 |
| 0.0001 | 0.2 | 0.001 | 4 | 0.160623699426651 | 0.46549999713897705 | 5.683471202850342 |
| 1e-06 | 0.0 | 0.0001 | 4 | 0.15808594226837158 | 0.447299987077713 | 5.748147010803223 |
| 1e-06 | 0.2 | 0.0001 | 4 | 0.1569720059633255 | 0.44394001364707947 | 5.787014961242676 |
| 1e-05 | 0.2 | 0.0001 | 4 | 0.15495237708091736 | 0.4447399973869324 | 5.746347427368164 |
| 1e-05 | 0.0 | 0.0001 | 4 | 0.15416443347930908 | 0.444599986076355 | 5.699609756469727 |
| 0.0001 | 0.5 | 0.001 | 4 | 0.14642344415187836 | 0.4466399848461151 | 5.846370697021484 |
| 1e-05 | 0.5 | 0.0001 | 4 | 0.14460010826587677 | 0.4240800142288208 | 5.9642109870910645 |
| 1e-06 | 0.5 | 0.0001 | 4 | 0.14327310025691986 | 0.4229600131511688 | 6.001766681671143 |
| 0.0001 | 0.0 | 0.0001 | 4 | 0.13507984578609467 | 0.41255998611450195 | 5.8413872718811035 |
| 0.0001 | 0.2 | 0.0001 | 4 | 0.13298428058624268 | 0.412339985370636 | 5.88568115234375 |
| 0.0001 | 0.5 | 0.0001 | 4 | 0.1264013797044754 | 0.39706000685691833 | 6.085857391357422 |
| 1e-05 | 0.0 | 1e-05 | 4 | 0.07940536737442017 | 0.26151999831199646 | 6.94926118850708 |
| 1e-06 | 0.0 | 1e-05 | 4 | 0.0773734599351883 | 0.25982001423835754 | 6.9737725257873535 |
| 1e-05 | 0.2 | 1e-05 | 4 | 0.0767027884721756 | 0.2531000077724457 | 7.007055759429932 |
| 0.0001 | 0.0 | 1e-05 | 4 | 0.0765145868062973 | 0.2513599991798401 | 7.026798248291016 |
| 1e-06 | 0.2 | 1e-05 | 4 | 0.07485027611255646 | 0.2487799972295761 | 7.044581413269043 |
| 0.0001 | 0.2 | 1e-05 | 4 | 0.07289301604032516 | 0.24617999792099 | 7.121115684509277 |
| 1e-05 | 0.5 | 1e-05 | 4 | 0.06913117319345474 | 0.23270000517368317 | 7.222418785095215 |
| 1e-06 | 0.5 | 1e-05 | 4 | 0.06849869340658188 | 0.23136000335216522 | 7.259231090545654 |
| 0.0001 | 0.5 | 1e-05 | 4 | 0.06784408539533615 | 0.22910000383853912 | 7.360254764556885 |-->

## Key Findings:
* The best configuration for Recall@20:
  - weight_decay = 1e-06
  - dropout_rate = 0.2
  - learning_rate = 0.001
  - Achieves R@20 = 0.5052400231361389 and MRR@20 = 0.19707946479320526
* The best configuration for MRR@20:
  - weight_decay = 1e-06
  - dropout_rate = 0.2
  - learning_rate = 0.001
  - Achieves MRR@20 = 0.19707946479320526 and R@20 = 0.5052400231361389

Common patterns:
* Higher learning rate (0.001) consistently performs better
* Lower weight decay (1e-05 or 1e-06) yields better results
* Dropout rate impact is less significant when other parameters are optimal

The top configurations are very close in performance, with the main difference being the trade-off between slightly better recall or MRR.

## Graph With Embeddings and Attention

Sorted by Recall@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | epoch | R@20 | MRR@20 | Loss |
|-------------|--------------|---------------|-------|------|--------|------|
| 1e-05 | 0.2 | 0.001 | 4 | 0.5530200004577637 | 0.2538684904575348 | 5.189048767089844 |
| 1e-05 | 0.0 | 0.001 | 4 | 0.5494800209999084 | 0.2606692910194397 | 5.2505784034729 |
| 1e-06 | 0.2 | 0.001 | 4 | 0.5435199737548828 | 0.24483181536197662 | 5.340364456176758 |
| 0.0001 | 0.2 | 0.001 | 4 | 0.5424799919128418 | 0.24397100508213043 | 5.230758190155029 |
| 0.0001 | 0.0 | 0.001 | 4 | 0.5421800017356873 | 0.2517438232898712 | 5.229765892028809 |
<!--| 1e-05 | 0.5 | 0.001 | 4 | 0.5401800274848938 | 0.22935748100280762 | 5.2488789558410645 |
| 1e-06 | 0.0 | 0.001 | 4 | 0.5379800200462341 | 0.25257161259651184 | 5.416688919067383 |
| 0.0001 | 0.5 | 0.001 | 4 | 0.5331000089645386 | 0.2224811166524887 | 5.294073581695557 |
| 1e-06 | 0.5 | 0.001 | 4 | 0.525879979133606 | 0.2199414074420929 | 5.4408955574035645 |
| 1e-05 | 0.0 | 0.0001 | 4 | 0.4928399920463562 | 0.1997435986995697 | 5.450355052947998 |
| 1e-06 | 0.0 | 0.0001 | 4 | 0.49107998609542847 | 0.20137141644954681 | 5.524374008178711 |
| 1e-05 | 0.2 | 0.0001 | 4 | 0.48763999342918396 | 0.19222569465637207 | 5.485910415649414 |
| 1e-06 | 0.2 | 0.0001 | 4 | 0.48471999168395996 | 0.191301628947258 | 5.542597770690918 |
| 0.0001 | 0.0 | 0.0001 | 4 | 0.47964000701904297 | 0.1823107749223709 | 5.5116963386535645 |
| 1e-06 | 0.5 | 0.0001 | 4 | 0.4744400084018707 | 0.17862439155578613 | 5.584404945373535 |
| 0.0001 | 0.2 | 0.0001 | 4 | 0.4743399918079376 | 0.17457719147205353 | 5.543837547302246 |
| 1e-05 | 0.5 | 0.0001 | 4 | 0.4719800055027008 | 0.1743311583995819 | 5.5572896003723145 |
| 0.0001 | 0.5 | 0.0001 | 4 | 0.4660399854183197 | 0.16408787667751312 | 5.574777126312256 |
| 1e-06 | 0.2 | 1e-05 | 4 | 0.26065999269485474 | 0.07830517739057541 | 6.908157825469971 |
| 1e-06 | 0.0 | 1e-05 | 4 | 0.26061999797821045 | 0.0771152526140213 | 6.913190841674805 |
| 0.0001 | 0.0 | 1e-05 | 4 | 0.257999986410141 | 0.07581494748592377 | 6.960726261138916 |
| 1e-05 | 0.0 | 1e-05 | 4 | 0.2570599913597107 | 0.07677021622657776 | 6.919296741485596 |
| 1e-05 | 0.2 | 1e-05 | 4 | 0.2570199966430664 | 0.07740334421396255 | 6.911798000335693 |
| 1e-06 | 0.5 | 1e-05 | 4 | 0.25637999176979065 | 0.0766734704375267 | 6.972477912902832 |
| 1e-05 | 0.5 | 1e-05 | 4 | 0.25512000918388367 | 0.07416628301143646 | 6.938106536865234 |
| 0.0001 | 0.2 | 1e-05 | 4 | 0.24775999784469604 | 0.07338733226060867 | 7.009960174560547 |
| 0.0001 | 0.5 | 1e-05 | 4 | 0.24661999940872192 | 0.07098620384931564 | 7.008839130401611 |-->

Sorted by MRR@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | epoch | MRR@20 | R@20 | Loss |
|-------------|--------------|---------------|-------|--------|------|------|
| 1e-05 | 0.0 | 0.001 | 4 | 0.2606692910194397 | 0.5494800209999084 | 5.2505784034729 |
| 1e-05 | 0.2 | 0.001 | 4 | 0.2538684904575348 | 0.5530200004577637 | 5.189048767089844 |
| 1e-06 | 0.0 | 0.001 | 4 | 0.25257161259651184 | 0.5379800200462341 | 5.416688919067383 |
| 0.0001 | 0.0 | 0.001 | 4 | 0.2517438232898712 | 0.5421800017356873 | 5.229765892028809 |
| 1e-06 | 0.2 | 0.001 | 4 | 0.24483181536197662 | 0.5435199737548828 | 5.340364456176758 |
<!--| 0.0001 | 0.2 | 0.001 | 4 | 0.24397100508213043 | 0.5424799919128418 | 5.230758190155029 |
| 1e-05 | 0.5 | 0.001 | 4 | 0.22935748100280762 | 0.5401800274848938 | 5.2488789558410645 |
| 0.0001 | 0.5 | 0.001 | 4 | 0.2224811166524887 | 0.5331000089645386 | 5.294073581695557 |
| 1e-06 | 0.5 | 0.001 | 4 | 0.2199414074420929 | 0.525879979133606 | 5.4408955574035645 |
| 1e-06 | 0.0 | 0.0001 | 4 | 0.20137141644954681 | 0.49107998609542847 | 5.524374008178711 |
| 1e-05 | 0.0 | 0.0001 | 4 | 0.1997435986995697 | 0.4928399920463562 | 5.450355052947998 |
| 1e-05 | 0.2 | 0.0001 | 4 | 0.19222569465637207 | 0.48763999342918396 | 5.485910415649414 |
| 1e-06 | 0.2 | 0.0001 | 4 | 0.191301628947258 | 0.48471999168395996 | 5.542597770690918 |
| 0.0001 | 0.0 | 0.0001 | 4 | 0.1823107749223709 | 0.47964000701904297 | 5.5116963386535645 |
| 1e-06 | 0.5 | 0.0001 | 4 | 0.17862439155578613 | 0.4744400084018707 | 5.584404945373535 |
| 0.0001 | 0.2 | 0.0001 | 4 | 0.17457719147205353 | 0.4743399918079376 | 5.543837547302246 |
| 1e-05 | 0.5 | 0.0001 | 4 | 0.1743311583995819 | 0.4719800055027008 | 5.5572896003723145 |
| 0.0001 | 0.5 | 0.0001 | 4 | 0.16408787667751312 | 0.4660399854183197 | 5.574777126312256 |
| 1e-06 | 0.2 | 1e-05 | 4 | 0.07830517739057541 | 0.26065999269485474 | 6.908157825469971 |
| 1e-05 | 0.2 | 1e-05 | 4 | 0.07740334421396255 | 0.2570199966430664 | 6.911798000335693 |
| 1e-06 | 0.0 | 1e-05 | 4 | 0.0771152526140213 | 0.26061999797821045 | 6.913190841674805 |
| 1e-05 | 0.0 | 1e-05 | 4 | 0.07677021622657776 | 0.2570599913597107 | 6.919296741485596 |
| 1e-06 | 0.5 | 1e-05 | 4 | 0.0766734704375267 | 0.25637999176979065 | 6.972477912902832 |
| 0.0001 | 0.0 | 1e-05 | 4 | 0.07581494748592377 | 0.257999986410141 | 6.960726261138916 |
| 1e-05 | 0.5 | 1e-05 | 4 | 0.07416628301143646 | 0.25512000918388367 | 6.938106536865234 |
| 0.0001 | 0.2 | 1e-05 | 4 | 0.07338733226060867 | 0.24775999784469604 | 7.009960174560547 |
| 0.0001 | 0.5 | 1e-05 | 4 | 0.07098620384931564 | 0.24661999940872192 | 7.008839130401611 |-->

## Key Findings:
* The best configuration for Recall@20:
  - weight_decay = 1e-05
  - dropout_rate = 0.2
  - learning_rate = 0.001
  - Achieves R@20 = 0.5530200004577637 and MRR@20 = 0.2538684904575348
* The best configuration for MRR@20:
  - weight_decay = 1e-05
  - dropout_rate = 0.0
  - learning_rate = 0.001
  - Achieves MRR@20 = 0.2606692910194397 and R@20 = 0.5494800209999084

* The best configuration achieves better results than running experiment 5 (weight_decay=0,0001, no dropout_rate) for 25 epochs with a scheduler to adjust the lr. The results for experiment 6 are:
  - R@20=0.5706
  - P@20=0.0285

## Common Patterns:
* Higher learning rate (0.001) consistently performs better
* Lower weight decay (1e-05 or 1e-06) yields better results
* Dropout rate impact is less significant when other parameters are optimal

## Graph With Embeddings and Attentional Aggregation

Sorted by Recall@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | epoch | R@20 | MRR@20 | Loss |
|-------------|--------------|---------------|-------|------|--------|------|
| 1e-05 | 0.5 | 0.001 | 4 | 0.5768399834632874 | 0.2954697012901306 | 4.997600555419922 |
| 1e-05 | 0.2 | 0.001 | 4 | 0.5755000114440918 | 0.2934810519218445 | 5.03712272644043 |
| 1e-06 | 0.5 | 0.001 | 4 | 0.5744199752807617 | 0.29744043946266174 | 5.087275981903076 |
| 1e-05 | 0.0 | 0.001 | 4 | 0.5708799958229065 | 0.2885268032550812 | 5.090220928192139 |
| 1e-06 | 0.2 | 0.001 | 4 | 0.5680199861526489 | 0.2910313308238983 | 5.203141212463379 |
<!--| 0.0001 | 0.2 | 0.001 | 4 | 0.5588800311088562 | 0.2719123661518097 | 5.121089935302734 |
| 0.0001 | 0.0 | 0.001 | 4 | 0.5571799874305725 | 0.27078890800476074 | 5.097511291503906 |
| 0.0001 | 0.5 | 0.001 | 4 | 0.5562599897384644 | 0.2694658935070038 | 5.144231796264648 |
| 1e-06 | 0.0 | 0.001 | 4 | 0.5561000108718872 | 0.284104585647583 | 5.340958595275879 |
| 1e-06 | 0.5 | 0.0001 | 4 | 0.5466399788856506 | 0.28672561049461365 | 5.153590679168701 |
| 1e-06 | 0.2 | 0.0001 | 4 | 0.5444200038909912 | 0.28636422753334045 | 5.156632900238037 |
| 1e-05 | 0.5 | 0.0001 | 4 | 0.5420799851417542 | 0.27691835165023804 | 5.135283946990967 |
| 1e-06 | 0.0 | 0.0001 | 4 | 0.5400800108909607 | 0.28213784098625183 | 5.197235584259033 |
| 1e-05 | 0.2 | 0.0001 | 4 | 0.5377600193023682 | 0.2729479968547821 | 5.163045406341553 |
| 1e-05 | 0.0 | 0.0001 | 4 | 0.5360599756240845 | 0.2719234526157379 | 5.165491104125977 |
| 0.0001 | 0.2 | 0.0001 | 4 | 0.5154799818992615 | 0.23866376280784607 | 5.290660858154297 |
| 0.0001 | 0.0 | 0.0001 | 4 | 0.5115200281143188 | 0.23711305856704712 | 5.304518222808838 |
| 0.0001 | 0.5 | 0.0001 | 4 | 0.5093799829483032 | 0.2334834784269333 | 5.346713542938232 |
| 1e-06 | 0.0 | 1e-05 | 4 | 0.3387199938297272 | 0.1323675662279129 | 6.529656887054443 |
| 1e-05 | 0.0 | 1e-05 | 4 | 0.3377799987792969 | 0.1341569423675537 | 6.497056007385254 |
| 1e-05 | 0.2 | 1e-05 | 4 | 0.33474001288414 | 0.13108128309249878 | 6.522027492523193 |
| 1e-06 | 0.2 | 1e-05 | 4 | 0.3336400091648102 | 0.13058744370937347 | 6.5485520362854 |
| 0.0001 | 0.0 | 1e-05 | 4 | 0.33070001006126404 | 0.12909500300884247 | 6.5713677406311035 |
| 0.0001 | 0.2 | 1e-05 | 4 | 0.3255000114440918 | 0.12549977004528046 | 6.607359409332275 |
| 1e-06 | 0.5 | 1e-05 | 4 | 0.3245599865913391 | 0.12253430485725403 | 6.630266189575195 |
| 1e-05 | 0.5 | 1e-05 | 4 | 0.32359999418258667 | 0.12493772804737091 | 6.590766429901123 |
| 0.0001 | 0.5 | 1e-05 | 4 | 0.31856000423431396 | 0.12075486779212952 | 6.662722110748291 |-->

Sorted by MRR@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | epoch | MRR@20 | R@20 | Loss |
|-------------|--------------|---------------|-------|--------|------|------|
| 1e-06 | 0.5 | 0.001 | 4 | 0.29744043946266174 | 0.5744199752807617 | 5.087275981903076 |
| 1e-05 | 0.5 | 0.001 | 4 | 0.2954697012901306 | 0.5768399834632874 | 4.997600555419922 |
| 1e-05 | 0.2 | 0.001 | 4 | 0.2934810519218445 | 0.5755000114440918 | 5.03712272644043 |
| 1e-06 | 0.2 | 0.001 | 4 | 0.2910313308238983 | 0.5680199861526489 | 5.203141212463379 |
| 1e-05 | 0.0 | 0.001 | 4 | 0.2885268032550812 | 0.5708799958229065 | 5.090220928192139 |
<!--| 1e-06 | 0.5 | 0.0001 | 4 | 0.28672561049461365 | 0.5466399788856506 | 5.153590679168701 |
| 1e-06 | 0.2 | 0.0001 | 4 | 0.28636422753334045 | 0.5444200038909912 | 5.156632900238037 |
| 1e-06 | 0.0 | 0.001 | 4 | 0.284104585647583 | 0.5561000108718872 | 5.340958595275879 |
| 1e-06 | 0.0 | 0.0001 | 4 | 0.28213784098625183 | 0.5400800108909607 | 5.197235584259033 |
| 1e-05 | 0.5 | 0.0001 | 4 | 0.27691835165023804 | 0.5420799851417542 | 5.135283946990967 |
| 1e-05 | 0.2 | 0.0001 | 4 | 0.2729479968547821 | 0.5377600193023682 | 5.163045406341553 |
| 1e-05 | 0.0 | 0.0001 | 4 | 0.2719234526157379 | 0.5360599756240845 | 5.165491104125977 |
| 0.0001 | 0.2 | 0.001 | 4 | 0.2719123661518097 | 0.5588800311088562 | 5.121089935302734 |
| 0.0001 | 0.0 | 0.001 | 4 | 0.27078890800476074 | 0.5571799874305725 | 5.097511291503906 |
| 0.0001 | 0.5 | 0.001 | 4 | 0.2694658935070038 | 0.5562599897384644 | 5.144231796264648 |
| 0.0001 | 0.2 | 0.0001 | 4 | 0.23866376280784607 | 0.5154799818992615 | 5.290660858154297 |
| 0.0001 | 0.0 | 0.0001 | 4 | 0.23711305856704712 | 0.5115200281143188 | 5.304518222808838 |
| 0.0001 | 0.5 | 0.0001 | 4 | 0.2334834784269333 | 0.5093799829483032 | 5.346713542938232 |
| 1e-05 | 0.0 | 1e-05 | 4 | 0.1341569423675537 | 0.3377799987792969 | 6.497056007385254 |
| 1e-06 | 0.0 | 1e-05 | 4 | 0.1323675662279129 | 0.3387199938297272 | 6.529656887054443 |
| 1e-05 | 0.2 | 1e-05 | 4 | 0.13108128309249878 | 0.33474001288414 | 6.522027492523193 |
| 1e-06 | 0.2 | 1e-05 | 4 | 0.13058744370937347 | 0.3336400091648102 | 6.5485520362854 |
| 0.0001 | 0.0 | 1e-05 | 4 | 0.12909500300884247 | 0.33070001006126404 | 6.5713677406311035 |
| 0.0001 | 0.2 | 1e-05 | 4 | 0.12549977004528046 | 0.3255000114440918 | 6.607359409332275 |
| 1e-05 | 0.5 | 1e-05 | 4 | 0.12493772804737091 | 0.32359999418258667 | 6.590766429901123 |
| 1e-06 | 0.5 | 1e-05 | 4 | 0.12253430485725403 | 0.3245599865913391 | 6.630266189575195 |
| 0.0001 | 0.5 | 1e-05 | 4 | 0.12075486779212952 | 0.31856000423431396 | 6.662722110748291 |-->

## Key Findings:
* The best configuration for Recall@20:
  - weight_decay = 1e-05
  - dropout_rate = 0.5
  - learning_rate = 0.001
  - Achieves R@20 = 0.5768399834632874 and MRR@20 = 0.2954697012901306
* The best configuration for MRR@20:
  - weight_decay = 1e-06
  - dropout_rate = 0.5
  - learning_rate = 0.001
  - Achieves MRR@20 = 0.29744043946266174 and R@20 = 0.5744199752807617
* The best configuration achieves better results than running experiment 6 (weight_decay=0,0001, dropout_rate=0.3) for 25 epochs with a scheduler to adjust the lr. The results for experiment 6 are:
  - R@20: 0.5980
  - MRR@20: 0.3119



## Common Patterns:
* Higher learning rate (0.001) consistently performs better
* Lower weight decay (1e-05 or 1e-06) yields better results
* Dropout rate impact is less significant when other parameters are optimal
