# Experiments report

## Graph With Embeddings

Sorted by Recall@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | R@20 | P@20 | MRR@20 | Loss |
|-------------|--------------|---------------|------|------|---------|------|
| 1e-05 | 0.0 | 0.001 | 0.4964 | 0.0248 | 0.1916 | 5.5573 |
| 1e-05 | 0.5 | 0.001 | 0.4960 | 0.0248 | 0.1931 | 5.5442 |
| 1e-06 | 0.5 | 0.001 | 0.4910 | 0.0246 | 0.1943 | 5.7627 |
| 1e-06 | 0.0 | 0.001 | 0.4904 | 0.0245 | 0.1926 | 5.7478 |
| 1e-06 | 0.2 | 0.001 | 0.4909 | 0.0245 | 0.1926 | 5.7427 |
<!--| 1e-05 | 0.2 | 0.001 | 0.4974 | 0.0249 | 0.1916 | 5.5410 |
| 0.0001 | 0.0 | 0.001 | 0.4737 | 0.0237 | 0.1676 | 5.6293 |
| 0.0001 | 0.5 | 0.001 | 0.4707 | 0.0235 | 0.1667 | 5.6265 |
| 0.0001 | 0.2 | 0.001 | 0.4681 | 0.0234 | 0.1644 | 5.6404 |
| 1e-06 | 0.0 | 0.0001 | 0.4460 | 0.0223 | 0.1569 | 5.7470 |
| 1e-06 | 0.5 | 0.0001 | 0.4473 | 0.0224 | 0.1575 | 5.7527 |
| 1e-06 | 0.2 | 0.0001 | 0.4455 | 0.0223 | 0.1573 | 5.7503 |
| 1e-05 | 0.0 | 0.0001 | 0.4422 | 0.0221 | 0.1534 | 5.7087 |
| 1e-05 | 0.5 | 0.0001 | 0.4430 | 0.0222 | 0.1529 | 5.7103 |
| 1e-05 | 0.2 | 0.0001 | 0.4402 | 0.0220 | 0.1525 | 5.7157 |
| 0.0001 | 0.0 | 0.0001 | 0.4154 | 0.0208 | 0.1343 | 5.8321 |
| 0.0001 | 0.2 | 0.0001 | 0.4139 | 0.0207 | 0.1355 | 5.8342 |
| 0.0001 | 0.5 | 0.0001 | 0.4126 | 0.0206 | 0.1337 | 5.8342 |
| 1e-06 | 0.0 | 1e-05 | 0.2620 | 0.0131 | 0.0791 | 6.9571 |
| 1e-05 | 0.2 | 1e-05 | 0.2606 | 0.0130 | 0.0773 | 6.9478 |
| 1e-06 | 0.2 | 1e-05 | 0.2588 | 0.0129 | 0.0791 | 6.9607 |
| 1e-05 | 0.0 | 1e-05 | 0.2573 | 0.0129 | 0.0760 | 6.9604 |
| 1e-06 | 0.5 | 1e-05 | 0.2565 | 0.0128 | 0.0764 | 6.9800 |
| 1e-05 | 0.5 | 1e-05 | 0.2559 | 0.0128 | 0.0755 | 6.9457 |
| 0.0001 | 0.2 | 1e-05 | 0.2525 | 0.0126 | 0.0739 | 7.0286 |
| 0.0001 | 0.5 | 1e-05 | 0.2506 | 0.0125 | 0.0738 | 7.0368 |
| 0.0001 | 0.0 | 1e-05 | 0.2487 | 0.0124 | 0.0744 | 7.0491 |-->


Sorted by MRR@20 (highest to lowest, top 5):
| weight_decay | dropout_rate | learning_rate | MRR@20 | R@20 | P@20 | Loss |
|-------------|--------------|---------------|---------|------|------|------|
| 1e-06 | 0.5 | 0.001 | 0.1943 | 0.4910 | 0.0246 | 5.7627 |
| 1e-06 | 0.0 | 0.001 | 0.1926 | 0.4904 | 0.0245 | 5.7478 |
| 1e-06 | 0.2 | 0.001 | 0.1926 | 0.4909 | 0.0245 | 5.7427 |
| 1e-05 | 0.5 | 0.001 | 0.1931 | 0.4960 | 0.0248 | 5.5442 |
| 1e-05 | 0.0 | 0.001 | 0.1916 | 0.4964 | 0.0248 | 5.5573 |
<!--| 1e-05 | 0.2 | 0.001 | 0.1916 | 0.4974 | 0.0249 | 5.5410 |
| 0.0001 | 0.0 | 0.001 | 0.1676 | 0.4737 | 0.0237 | 5.6293 |
| 0.0001 | 0.5 | 0.001 | 0.1667 | 0.4707 | 0.0235 | 5.6265 |
| 0.0001 | 0.2 | 0.001 | 0.1644 | 0.4681 | 0.0234 | 5.6404 |
| 1e-06 | 0.0 | 0.0001 | 0.1569 | 0.4460 | 0.0223 | 5.7470 |
| 1e-06 | 0.5 | 0.0001 | 0.1575 | 0.4473 | 0.0224 | 5.7527 |
| 1e-06 | 0.2 | 0.0001 | 0.1573 | 0.4455 | 0.0223 | 5.7503 |
| 1e-05 | 0.0 | 0.0001 | 0.1534 | 0.4422 | 0.0221 | 5.7087 |
| 1e-05 | 0.5 | 0.0001 | 0.1529 | 0.4430 | 0.0222 | 5.7103 |
| 1e-05 | 0.2 | 0.0001 | 0.1525 | 0.4402 | 0.0220 | 5.7157 |
| 0.0001 | 0.2 | 0.0001 | 0.1355 | 0.4139 | 0.0207 | 5.8342 |
| 0.0001 | 0.0 | 0.0001 | 0.1343 | 0.4154 | 0.0208 | 5.8321 |
| 0.0001 | 0.5 | 0.0001 | 0.1337 | 0.4126 | 0.0206 | 5.8342 |
| 1e-06 | 0.0 | 1e-05 | 0.0791 | 0.2620 | 0.0131 | 6.9571 |
| 1e-06 | 0.2 | 1e-05 | 0.0791 | 0.2588 | 0.0129 | 6.9607 |
| 1e-06 | 0.5 | 1e-05 | 0.0764 | 0.2565 | 0.0128 | 6.9800 |
| 1e-05 | 0.0 | 1e-05 | 0.0760 | 0.2573 | 0.0129 | 6.9604 |
| 1e-05 | 0.5 | 1e-05 | 0.0755 | 0.2559 | 0.0128 | 6.9457 |
| 1e-05 | 0.2 | 1e-05 | 0.0773 | 0.2606 | 0.0130 | 6.9478 |
| 0.0001 | 0.0 | 1e-05 | 0.0744 | 0.2487 | 0.0124 | 7.0491 |
| 0.0001 | 0.2 | 1e-05 | 0.0739 | 0.2525 | 0.0126 | 7.0286 |
| 0.0001 | 0.5 | 1e-05 | 0.0738 | 0.2506 | 0.0125 | 7.0368 |-->

Key findings:
* Best configuration for R@20:
    * weight_decay = 1e-05
    * dropout_rate = 0.2
    * learning_rate = 0.001
    * Achieves R@20 = 0.4974 and MRR@20 = 0.1916
* Best configuration for MRR@20:
    * weight_decay = 1e-06
    * dropout_rate = 0.5
    * learning_rate = 0.001
    * Achieves MRR@20 = 0.1943 and R@20 = 0.4910

Common patterns:
* Higher learning rate (0.001) consistently performs better
* Lower weight decay (1e-05 or 1e-06) yields better results
* Dropout rate impact is less significant when other parameters are optimal

The top configurations are very close in performance, with the main difference being the trade-off between slightly better recall or MRR.

## Graph With Embeddings and Attention

Sorted by Recall@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | epoch | R@20 | MRR@20 | Loss |
|-------------|--------------|---------------|-------|------|--------|------|
| 1e-05 | 0.2 | 0.001 | 4 | 0.5530200004577637 | 0.2538684904575348 | 5.189048767089844 |
| 1e-05 | 0.0 | 0.001 | 4 | 0.5494800209999084 | 0.2606692910194397 | 5.2505784034729 |
| 1e-06 | 0.2 | 0.001 | 4 | 0.5435199737548828 | 0.24483181536197662 | 5.340364456176758 |
| 0.0001 | 0.2 | 0.001 | 4 | 0.5424799919128418 | 0.24397100508213043 | 5.230758190155029 |
| 0.0001 | 0.0 | 0.001 | 4 | 0.5421800017356873 | 0.2517438232898712 | 5.229765892028809 |
<!--| 1e-05 | 0.5 | 0.001 | 4 | 0.5401800274848938 | 0.22935748100280762 | 5.2488789558410645 |
| 1e-06 | 0.0 | 0.001 | 4 | 0.5379800200462341 | 0.25257161259651184 | 5.416688919067383 |
| 0.0001 | 0.5 | 0.001 | 4 | 0.5331000089645386 | 0.2224811166524887 | 5.294073581695557 |
| 1e-06 | 0.5 | 0.001 | 4 | 0.525879979133606 | 0.2199414074420929 | 5.4408955574035645 |
| 1e-05 | 0.0 | 0.0001 | 4 | 0.4928399920463562 | 0.1997435986995697 | 5.450355052947998 |
| 1e-06 | 0.0 | 0.0001 | 4 | 0.49107998609542847 | 0.20137141644954681 | 5.524374008178711 |
| 1e-05 | 0.2 | 0.0001 | 4 | 0.48763999342918396 | 0.19222569465637207 | 5.485910415649414 |
| 1e-06 | 0.2 | 0.0001 | 4 | 0.48471999168395996 | 0.191301628947258 | 5.542597770690918 |
| 0.0001 | 0.0 | 0.0001 | 4 | 0.47964000701904297 | 0.1823107749223709 | 5.5116963386535645 |
| 1e-06 | 0.5 | 0.0001 | 4 | 0.4744400084018707 | 0.17862439155578613 | 5.584404945373535 |
| 0.0001 | 0.2 | 0.0001 | 4 | 0.4743399918079376 | 0.17457719147205353 | 5.543837547302246 |
| 1e-05 | 0.5 | 0.0001 | 4 | 0.4719800055027008 | 0.1743311583995819 | 5.5572896003723145 |
| 0.0001 | 0.5 | 0.0001 | 4 | 0.4660399854183197 | 0.16408787667751312 | 5.574777126312256 |
| 1e-06 | 0.2 | 1e-05 | 4 | 0.26065999269485474 | 0.07830517739057541 | 6.908157825469971 |
| 1e-06 | 0.0 | 1e-05 | 4 | 0.26061999797821045 | 0.0771152526140213 | 6.913190841674805 |
| 0.0001 | 0.0 | 1e-05 | 4 | 0.257999986410141 | 0.07581494748592377 | 6.960726261138916 |
| 1e-05 | 0.0 | 1e-05 | 4 | 0.2570599913597107 | 0.07677021622657776 | 6.919296741485596 |
| 1e-05 | 0.2 | 1e-05 | 4 | 0.2570199966430664 | 0.07740334421396255 | 6.911798000335693 |
| 1e-06 | 0.5 | 1e-05 | 4 | 0.25637999176979065 | 0.0766734704375267 | 6.972477912902832 |
| 1e-05 | 0.5 | 1e-05 | 4 | 0.25512000918388367 | 0.07416628301143646 | 6.938106536865234 |
| 0.0001 | 0.2 | 1e-05 | 4 | 0.24775999784469604 | 0.07338733226060867 | 7.009960174560547 |
| 0.0001 | 0.5 | 1e-05 | 4 | 0.24661999940872192 | 0.07098620384931564 | 7.008839130401611 |-->

Sorted by MRR@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | epoch | MRR@20 | R@20 | Loss |
|-------------|--------------|---------------|-------|--------|------|------|
| 1e-05 | 0.0 | 0.001 | 4 | 0.2606692910194397 | 0.5494800209999084 | 5.2505784034729 |
| 1e-05 | 0.2 | 0.001 | 4 | 0.2538684904575348 | 0.5530200004577637 | 5.189048767089844 |
| 1e-06 | 0.0 | 0.001 | 4 | 0.25257161259651184 | 0.5379800200462341 | 5.416688919067383 |
| 0.0001 | 0.0 | 0.001 | 4 | 0.2517438232898712 | 0.5421800017356873 | 5.229765892028809 |
| 1e-06 | 0.2 | 0.001 | 4 | 0.24483181536197662 | 0.5435199737548828 | 5.340364456176758 |
<!--| 0.0001 | 0.2 | 0.001 | 4 | 0.24397100508213043 | 0.5424799919128418 | 5.230758190155029 |
| 1e-05 | 0.5 | 0.001 | 4 | 0.22935748100280762 | 0.5401800274848938 | 5.2488789558410645 |
| 0.0001 | 0.5 | 0.001 | 4 | 0.2224811166524887 | 0.5331000089645386 | 5.294073581695557 |
| 1e-06 | 0.5 | 0.001 | 4 | 0.2199414074420929 | 0.525879979133606 | 5.4408955574035645 |
| 1e-06 | 0.0 | 0.0001 | 4 | 0.20137141644954681 | 0.49107998609542847 | 5.524374008178711 |
| 1e-05 | 0.0 | 0.0001 | 4 | 0.1997435986995697 | 0.4928399920463562 | 5.450355052947998 |
| 1e-05 | 0.2 | 0.0001 | 4 | 0.19222569465637207 | 0.48763999342918396 | 5.485910415649414 |
| 1e-06 | 0.2 | 0.0001 | 4 | 0.191301628947258 | 0.48471999168395996 | 5.542597770690918 |
| 0.0001 | 0.0 | 0.0001 | 4 | 0.1823107749223709 | 0.47964000701904297 | 5.5116963386535645 |
| 1e-06 | 0.5 | 0.0001 | 4 | 0.17862439155578613 | 0.4744400084018707 | 5.584404945373535 |
| 0.0001 | 0.2 | 0.0001 | 4 | 0.17457719147205353 | 0.4743399918079376 | 5.543837547302246 |
| 1e-05 | 0.5 | 0.0001 | 4 | 0.1743311583995819 | 0.4719800055027008 | 5.5572896003723145 |
| 0.0001 | 0.5 | 0.0001 | 4 | 0.16408787667751312 | 0.4660399854183197 | 5.574777126312256 |
| 1e-06 | 0.2 | 1e-05 | 4 | 0.07830517739057541 | 0.26065999269485474 | 6.908157825469971 |
| 1e-05 | 0.2 | 1e-05 | 4 | 0.07740334421396255 | 0.2570199966430664 | 6.911798000335693 |
| 1e-06 | 0.0 | 1e-05 | 4 | 0.0771152526140213 | 0.26061999797821045 | 6.913190841674805 |
| 1e-05 | 0.0 | 1e-05 | 4 | 0.07677021622657776 | 0.2570599913597107 | 6.919296741485596 |
| 1e-06 | 0.5 | 1e-05 | 4 | 0.0766734704375267 | 0.25637999176979065 | 6.972477912902832 |
| 0.0001 | 0.0 | 1e-05 | 4 | 0.07581494748592377 | 0.257999986410141 | 6.960726261138916 |
| 1e-05 | 0.5 | 1e-05 | 4 | 0.07416628301143646 | 0.25512000918388367 | 6.938106536865234 |
| 0.0001 | 0.2 | 1e-05 | 4 | 0.07338733226060867 | 0.24775999784469604 | 7.009960174560547 |
| 0.0001 | 0.5 | 1e-05 | 4 | 0.07098620384931564 | 0.24661999940872192 | 7.008839130401611 |-->

## Key Findings:
* The best configuration for Recall@20:
  - weight_decay = 1e-05
  - dropout_rate = 0.2
  - learning_rate = 0.001
  - Achieves R@20 = 0.5530200004577637 and MRR@20 = 0.2538684904575348
* The best configuration for MRR@20:
  - weight_decay = 1e-05
  - dropout_rate = 0.0
  - learning_rate = 0.001
  - Achieves MRR@20 = 0.2606692910194397 and R@20 = 0.5494800209999084

* The best configuration achieves better results than running experiment 5 (weight_decay=0,0001, no dropout_rate) for 25 epochs with a scheduler to adjust the lr. The results for experiment 6 are:
  - R@20=0.5706
  - P@20=0.0285

## Common Patterns:
* Higher learning rate (0.001) consistently performs better
* Lower weight decay (1e-05 or 1e-06) yields better results
* Dropout rate impact is less significant when other parameters are optimal

## Graph With Embeddings and Attentional Aggregation

Sorted by Recall@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | epoch | R@20 | MRR@20 | Loss |
|-------------|--------------|---------------|-------|------|--------|------|
| 1e-05 | 0.5 | 0.001 | 4 | 0.5768399834632874 | 0.2954697012901306 | 4.997600555419922 |
| 1e-05 | 0.2 | 0.001 | 4 | 0.5755000114440918 | 0.2934810519218445 | 5.03712272644043 |
| 1e-06 | 0.5 | 0.001 | 4 | 0.5744199752807617 | 0.29744043946266174 | 5.087275981903076 |
| 1e-05 | 0.0 | 0.001 | 4 | 0.5708799958229065 | 0.2885268032550812 | 5.090220928192139 |
| 1e-06 | 0.2 | 0.001 | 4 | 0.5680199861526489 | 0.2910313308238983 | 5.203141212463379 |
<!--| 0.0001 | 0.2 | 0.001 | 4 | 0.5588800311088562 | 0.2719123661518097 | 5.121089935302734 |
| 0.0001 | 0.0 | 0.001 | 4 | 0.5571799874305725 | 0.27078890800476074 | 5.097511291503906 |
| 0.0001 | 0.5 | 0.001 | 4 | 0.5562599897384644 | 0.2694658935070038 | 5.144231796264648 |
| 1e-06 | 0.0 | 0.001 | 4 | 0.5561000108718872 | 0.284104585647583 | 5.340958595275879 |
| 1e-06 | 0.5 | 0.0001 | 4 | 0.5466399788856506 | 0.28672561049461365 | 5.153590679168701 |
| 1e-06 | 0.2 | 0.0001 | 4 | 0.5444200038909912 | 0.28636422753334045 | 5.156632900238037 |
| 1e-05 | 0.5 | 0.0001 | 4 | 0.5420799851417542 | 0.27691835165023804 | 5.135283946990967 |
| 1e-06 | 0.0 | 0.0001 | 4 | 0.5400800108909607 | 0.28213784098625183 | 5.197235584259033 |
| 1e-05 | 0.2 | 0.0001 | 4 | 0.5377600193023682 | 0.2729479968547821 | 5.163045406341553 |
| 1e-05 | 0.0 | 0.0001 | 4 | 0.5360599756240845 | 0.2719234526157379 | 5.165491104125977 |
| 0.0001 | 0.2 | 0.0001 | 4 | 0.5154799818992615 | 0.23866376280784607 | 5.290660858154297 |
| 0.0001 | 0.0 | 0.0001 | 4 | 0.5115200281143188 | 0.23711305856704712 | 5.304518222808838 |
| 0.0001 | 0.5 | 0.0001 | 4 | 0.5093799829483032 | 0.2334834784269333 | 5.346713542938232 |
| 1e-06 | 0.0 | 1e-05 | 4 | 0.3387199938297272 | 0.1323675662279129 | 6.529656887054443 |
| 1e-05 | 0.0 | 1e-05 | 4 | 0.3377799987792969 | 0.1341569423675537 | 6.497056007385254 |
| 1e-05 | 0.2 | 1e-05 | 4 | 0.33474001288414 | 0.13108128309249878 | 6.522027492523193 |
| 1e-06 | 0.2 | 1e-05 | 4 | 0.3336400091648102 | 0.13058744370937347 | 6.5485520362854 |
| 0.0001 | 0.0 | 1e-05 | 4 | 0.33070001006126404 | 0.12909500300884247 | 6.5713677406311035 |
| 0.0001 | 0.2 | 1e-05 | 4 | 0.3255000114440918 | 0.12549977004528046 | 6.607359409332275 |
| 1e-06 | 0.5 | 1e-05 | 4 | 0.3245599865913391 | 0.12253430485725403 | 6.630266189575195 |
| 1e-05 | 0.5 | 1e-05 | 4 | 0.32359999418258667 | 0.12493772804737091 | 6.590766429901123 |
| 0.0001 | 0.5 | 1e-05 | 4 | 0.31856000423431396 | 0.12075486779212952 | 6.662722110748291 |-->

Sorted by MRR@20 (highest to lowest, top 5):

| weight_decay | dropout_rate | learning_rate | epoch | MRR@20 | R@20 | Loss |
|-------------|--------------|---------------|-------|--------|------|------|
| 1e-06 | 0.5 | 0.001 | 4 | 0.29744043946266174 | 0.5744199752807617 | 5.087275981903076 |
| 1e-05 | 0.5 | 0.001 | 4 | 0.2954697012901306 | 0.5768399834632874 | 4.997600555419922 |
| 1e-05 | 0.2 | 0.001 | 4 | 0.2934810519218445 | 0.5755000114440918 | 5.03712272644043 |
| 1e-06 | 0.2 | 0.001 | 4 | 0.2910313308238983 | 0.5680199861526489 | 5.203141212463379 |
| 1e-05 | 0.0 | 0.001 | 4 | 0.2885268032550812 | 0.5708799958229065 | 5.090220928192139 |
<!--| 1e-06 | 0.5 | 0.0001 | 4 | 0.28672561049461365 | 0.5466399788856506 | 5.153590679168701 |
| 1e-06 | 0.2 | 0.0001 | 4 | 0.28636422753334045 | 0.5444200038909912 | 5.156632900238037 |
| 1e-06 | 0.0 | 0.001 | 4 | 0.284104585647583 | 0.5561000108718872 | 5.340958595275879 |
| 1e-06 | 0.0 | 0.0001 | 4 | 0.28213784098625183 | 0.5400800108909607 | 5.197235584259033 |
| 1e-05 | 0.5 | 0.0001 | 4 | 0.27691835165023804 | 0.5420799851417542 | 5.135283946990967 |
| 1e-05 | 0.2 | 0.0001 | 4 | 0.2729479968547821 | 0.5377600193023682 | 5.163045406341553 |
| 1e-05 | 0.0 | 0.0001 | 4 | 0.2719234526157379 | 0.5360599756240845 | 5.165491104125977 |
| 0.0001 | 0.2 | 0.001 | 4 | 0.2719123661518097 | 0.5588800311088562 | 5.121089935302734 |
| 0.0001 | 0.0 | 0.001 | 4 | 0.27078890800476074 | 0.5571799874305725 | 5.097511291503906 |
| 0.0001 | 0.5 | 0.001 | 4 | 0.2694658935070038 | 0.5562599897384644 | 5.144231796264648 |
| 0.0001 | 0.2 | 0.0001 | 4 | 0.23866376280784607 | 0.5154799818992615 | 5.290660858154297 |
| 0.0001 | 0.0 | 0.0001 | 4 | 0.23711305856704712 | 0.5115200281143188 | 5.304518222808838 |
| 0.0001 | 0.5 | 0.0001 | 4 | 0.2334834784269333 | 0.5093799829483032 | 5.346713542938232 |
| 1e-05 | 0.0 | 1e-05 | 4 | 0.1341569423675537 | 0.3377799987792969 | 6.497056007385254 |
| 1e-06 | 0.0 | 1e-05 | 4 | 0.1323675662279129 | 0.3387199938297272 | 6.529656887054443 |
| 1e-05 | 0.2 | 1e-05 | 4 | 0.13108128309249878 | 0.33474001288414 | 6.522027492523193 |
| 1e-06 | 0.2 | 1e-05 | 4 | 0.13058744370937347 | 0.3336400091648102 | 6.5485520362854 |
| 0.0001 | 0.0 | 1e-05 | 4 | 0.12909500300884247 | 0.33070001006126404 | 6.5713677406311035 |
| 0.0001 | 0.2 | 1e-05 | 4 | 0.12549977004528046 | 0.3255000114440918 | 6.607359409332275 |
| 1e-05 | 0.5 | 1e-05 | 4 | 0.12493772804737091 | 0.32359999418258667 | 6.590766429901123 |
| 1e-06 | 0.5 | 1e-05 | 4 | 0.12253430485725403 | 0.3245599865913391 | 6.630266189575195 |
| 0.0001 | 0.5 | 1e-05 | 4 | 0.12075486779212952 | 0.31856000423431396 | 6.662722110748291 |-->

## Key Findings:
* The best configuration for Recall@20:
  - weight_decay = 1e-05
  - dropout_rate = 0.5
  - learning_rate = 0.001
  - Achieves R@20 = 0.5768399834632874 and MRR@20 = 0.2954697012901306
* The best configuration for MRR@20:
  - weight_decay = 1e-06
  - dropout_rate = 0.5
  - learning_rate = 0.001
  - Achieves MRR@20 = 0.29744043946266174 and R@20 = 0.5744199752807617
* The best configuration achieves better results than running experiment 6 (weight_decay=0,0001, dropout_rate=0.3) for 25 epochs with a scheduler to adjust the lr. The results for experiment 6 are:
  - R@20: 0.5980
  - MRR@20: 0.3119



## Common Patterns:
* Higher learning rate (0.001) consistently performs better
* Lower weight decay (1e-05 or 1e-06) yields better results
* Dropout rate impact is less significant when other parameters are optimal
